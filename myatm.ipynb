{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-driven machine learning focusing on feature selection\n",
    "\n",
    "The **PdM(Predictive Maintenance)** approach proposed in [14], although it is evaluated over a use case\n",
    "of automated teller machines (**ATMs**), is **general enough to be applied on any\n",
    "industrial scenario**, where error and failure logs are available. It follows a similar\n",
    "rationale as [10], but implicitly assumes that the **log types recorded are more\n",
    "commonly related to the targeted failure** (e.g., generated from software exceptions) and puts more **emphasis on feature generation and selection**. We will refer\n",
    "to this approach as **FSPdM**. Its main drawback is that it **cannot scale in the\n",
    "number of event types** that are present in the logs.\n",
    "\n",
    "The authors propose a configurable approach for the creation of the training\n",
    "and testing datasets and the formation of a binary classification problem. More\n",
    "specifically, the dataset is divided into partitions (named **Observation Windows\n",
    "(OW)**) and each OW is further divided into **daily segments**. Every OW, is fol-\n",
    "lowed by a **Prediction Window (PW)** (i.e. partition with daily segments), in\n",
    "which a fault is predicted to take place. The range from the beginning of each\n",
    "OW up to the end of the related PW defines a **training or testing instance**. The\n",
    "**labelling of an instance (i.e. classes: likely to fail, or not to fail)** depends on the\n",
    "existence of a ticket report inside the PW (i.e. if there is a ticket in the PW, the\n",
    "instance is considered positive (i.e. likely to fail)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10.Korvesis, P., Besseau, S., Vazirgiannis, M.: Predictive maintenance in aviation:\n",
    "Failure prediction from post \n",
    "ight reports. In: IEEE Int. Conf. on Data Engineering\n",
    "(ICDE). pp. 1414{1422 (2018)\n",
    "\n",
    "14.Wang, J., Li, C., Han, S., Sarkar, S., Zhou, X.: Predictive maintenance based on\n",
    "event-log analysis: A case study. IBM Journal of Research and Development 61(1),\n",
    "11{121 (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'CORElearn' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'plyr' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'data.table' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'randomForest' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'argparser' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'arules' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'arulesSequences' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'xgboost' was built under R version 3.6.3\"Warning message:\n",
      "\"package 'DiagrammeR' was built under R version 3.6.3\""
     ]
    }
   ],
   "source": [
    "#Make the necessary imports.\n",
    "\n",
    "suppressMessages(library(CORElearn))\n",
    "suppressMessages(library(dplyr))\n",
    "suppressMessages(library(plyr))\n",
    "suppressMessages(library(data.table))\n",
    "suppressMessages(library(randomForest))\n",
    "suppressMessages(library(ggplot2))\n",
    "suppressMessages(library(grid))\n",
    "suppressMessages(library(argparser))\n",
    "suppressMessages(library(arules))\n",
    "suppressMessages(library(arulesSequences))\n",
    "suppressMessages(library(xgboost))\n",
    "suppressMessages(library(DiagrammeR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make an argument parser named p and keep there the necessary variables.\n",
    "\n",
    "p <- arg_parser(\"Implementation of the IBMs ATM Predictor\")\n",
    "\n",
    "# Add a positional argument\n",
    "p <- add_argument(p, \"train\", help=\"training dataset\")\n",
    "p <- add_argument(p, \"test\", help=\"test dataset\")\n",
    "\n",
    "p <- add_argument(p, \"fet\", help=\"different types of the fault events\",default=11)\n",
    "p <- add_argument(p, \"tet\", help=\"type of the target fault events\",default=11)\n",
    "\n",
    "p <- add_argument(p, \"--X\", help=\"# of segments/sub-windows\", default=3)#3#1\n",
    "p <- add_argument(p, \"--M\", help=\"segment legth (in days)\", default=2)#2#1\n",
    "p <- add_argument(p, \"--Y\", help=\"length of the prediction window (in days)\", default=3)#3#2\n",
    "p <- add_argument(p, \"--Z\", help=\"length of the buffer window (in days)\", default=0)\n",
    "p <- add_argument(p, \"--N\", help=\"moving step (in days)\", default=2)#2#1\n",
    "p <- add_argument(p, \"--step\", help=\"feature selection decrease step\", default=5)#5#1\n",
    "\n",
    "p <- add_argument(p, \"--sup\", help=\"pattern features appriori support\", default=0.8)\n",
    "p <- add_argument(p, \"--conf\", help=\"pattern features confidence\", default=0.6)\n",
    "p <- add_argument(p, \"--fs\", help=\"apply feature selection\", default=TRUE)\n",
    "p <- add_argument(p, \"--pf\", help=\"use pattern features\", default=TRUE)#FALSE\n",
    "p <- add_argument(p, \"--sf\", help=\"use similarity feature\", default=TRUE)\n",
    "p <- add_argument(p, \"--plogic\", help=\"TRUE for custom pattern detection logic FALSE for paper's logic\", default=TRUE)\n",
    "p <- add_argument(p, \"--csv\", help=\"output for csv\", default=FALSE)\n",
    "p <- add_argument(p, \"--minwint\", help=\"min # of days before failure to expect a warning for true positive decision\", default=2)#2#3\n",
    "p <- add_argument(p, \"--maxwint\", help=\"max # of days before failure to expect a warning for true positive decision\", default=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NULL"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in parse_args(p, c(\"C:/Users/Public/ptyxiakh/training_my_dataset.csv\", : could not find function \"parse_args\"\n",
     "output_type": "error",
     "traceback": [
      "Error in parse_args(p, c(\"C:/Users/Public/ptyxiakh/training_my_dataset.csv\", : could not find function \"parse_args\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "#Define the necessary variables.\n",
    "\n",
    "argv = data.frame() #make a data frame named argv\n",
    "#if( length(commandArgs(trailingOnly = TRUE)) != 0){\n",
    "if(FALSE){\n",
    "  argv <- parse_args(p)\n",
    "} else {\n",
    "  #parse to argv the p's arguments as  argv <- parse_args(p,c(\"training dataset's path\",\"test dataset's path\",fet,tet))  \n",
    "  argv <- parse_args(p,c(\"C:/Users/Public/ptyxiakh/training_my_dataset2.csv\",\"C:/Users/Public/ptyxiakh/testing_my_dataset2.csv\",11,11))\n",
    "}\n",
    "\n",
    "#init the variables\n",
    "train_path=argv$train\n",
    "test_path=argv$test\n",
    "\n",
    "b_length = argv$fet\n",
    "target_event = argv$tet\n",
    "\n",
    "X = argv$X\n",
    "M = argv$M\n",
    "Y = argv$Y\n",
    "Z = argv$Z\n",
    "N = argv$N\n",
    "step = argv$step\n",
    "sup = argv$sup\n",
    "conf = argv$conf\n",
    "FEATURE_SELECTION = argv$fs\n",
    "PATTERN_FEATURES = argv$pf\n",
    "JACCARD_FEATURE = argv$sf\n",
    "PATTERN_CUSTOM = argv$plogic\n",
    "csv = argv$csv\n",
    "max_warning_interval = argv$maxwint\n",
    "min_warning_interval = argv$minwint\n",
    "\n",
    "print(\"The data frame argv is:\")\n",
    "print(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading function\n",
    "**function: read_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for reading the csv file and save it to a two column table.\n",
    "\n",
    "read_dataset <- function(path){\n",
    "  dataset = read.table(path, header = TRUE, sep = \",\", dec = \".\", comment.char = \"#\")\n",
    "  dataset[, 2]  <- as.numeric(dataset[, 2])\n",
    "  return(dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test set\n",
    "\n",
    "The recorded log types read from csv files.\n",
    "One csv file(at **train_path**) has the **training_set** and the other(at **test_path**) the **testing_set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The test_set and training_set looks like:\"\n",
      "  Timestamps Event_id\n",
      "1 2014-01-21        1\n",
      "2 2014-01-21        3\n",
      "3 2014-01-21        4\n",
      "4 2014-01-21        5\n",
      "5 2014-01-21        6\n",
      "6 2014-01-21        4\n"
     ]
    }
   ],
   "source": [
    "#Reading train and test set.\n",
    "\n",
    "training_set = read_dataset(train_path)\n",
    "test_set =  read_dataset(test_path)\n",
    "\n",
    "print(\"The test_set and training_set looks like:\")\n",
    "print(head(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for computing frequencies of events\n",
    "**1) function: create_episodes_list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating frequency vectors for each day.\n",
    "\n",
    "create_episodes_list <- function(ds2years,b_length){\n",
    "\n",
    "  #data.frame for episodes\n",
    "  episode_df <- data.frame(Timestamps=as.Date(character()),Event_id=integer())\n",
    "  \n",
    "  #Change ds2years(table) to episode_df(data frame)    \n",
    "    \n",
    "  #iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(ds2years)) {\n",
    "    #get the current row of ds2years(table of data set)\n",
    "    meas <- ds2years[i,]\n",
    "    #add it to data frame  \n",
    "    episode_df <- rbind(episode_df,data.frame(Timestamps=meas$Timestamps, Event_id=meas$Event_id))\n",
    "\n",
    "  }\n",
    "  #group by day\n",
    "  aggr_episode_df = aggregate(episode_df[ ,2], FUN=function(x){return(x)}, by=list(as.Date(episode_df$Timestamps, \"%Y-%m-%d\")))\n",
    "  \n",
    "  #binarize the frequncy vector(function: compute_frequency_vectors)\n",
    "  frequency_day_vectors = compute_frequency_vectors(aggr_episode_df,b_length)\n",
    "\n",
    "  return(frequency_day_vectors)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) function: compute_frequency_vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert event vectors to binary vectors\n",
    "\n",
    "compute_frequency_vectors <- function(aggr_episode_df,b_length){\n",
    "    \n",
    "  #data frame for binary frequency vectors  \n",
    "  freq_aggr_episode_df <- data.frame(matrix(ncol = b_length+1, nrow = 0))\n",
    "  \n",
    "  #x keeps the names of the columns. |Timestamps||e_1||e_2|...|e_b_length|  \n",
    "  x <- c(c(\"Timestamps\"), c(paste(\"e_\",c(1:b_length),sep = \"\")))\n",
    "\n",
    "  #iterate over every line(day) of the aggr_episode_df\n",
    "  for(i in 1:nrow(aggr_episode_df)) {\n",
    "      \n",
    "      #init a vector with b_length zeros\n",
    "      freq_vector = as.vector(integer(b_length))\n",
    "    \n",
    "      #get the current row of aggr_episode_df(frequency vector-data frame of data set)\n",
    "      seg <- aggr_episode_df[i,]\n",
    "    \n",
    "      #for every value(fault event) in the current line(that happened in the current day)\n",
    "      for(value in seg$x[[1]]){\n",
    "          #replace the 0 in freq_vector with 1 at \"value=fault event\" position \n",
    "          freq_vector[[value]] = length(which(seg$x[[1]] == value))\n",
    "      }\n",
    "    \n",
    "      #add a new line to the bin_aggr_epissode_df\n",
    "      #we use a matrix holding the elements of the new_data.frame as matrix is able to store variable of different data types\n",
    "      \n",
    "      date = as.Date(seg$Group.1[[1]])\n",
    "      new_df = data.frame(matrix(c(date, freq_vector),nrow=1,ncol=b_length+1))\n",
    "      freq_aggr_episode_df <- rbind(freq_aggr_episode_df,new_df)\n",
    "  }\n",
    "  #set column's name as x defines\n",
    "  colnames(freq_aggr_episode_df) <- x\n",
    "  \n",
    "  #set column \"Timestamps\" x to a Date: \"Y-m-d\" column  \n",
    "  freq_aggr_episode_df$Timestamps <- as.Date(freq_aggr_episode_df$Timestamps , origin=\"1970-01-01\")\n",
    "    \n",
    "  return(freq_aggr_episode_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"For training set head of frequency_day_vectors is:\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "1 2014-01-01   0   0   0   0   0   0   0   0   0    0    1\n",
      "2 2014-01-02   0   1   1   1   1   1   1   0   0    0    0\n",
      "3 2014-01-03   1   1   1   1   1   2   1   2   0    0    0\n",
      "4 2014-01-04   0   0   1   1   2   1   0   1   0    0    0\n",
      "5 2014-01-05   1   1   1   1   1   2   2   0   0    0    0\n",
      "6 2014-01-06   1   1   1   1   1   0   0   1   0    1    0\n"
     ]
    }
   ],
   "source": [
    "#Create for each of the training and  testing set a dataframe keeping for each day the frequency of the fault events.\n",
    "\n",
    "frequency_day_vectors = create_episodes_list(training_set,b_length)\n",
    "test_frequency_day_vectors = create_episodes_list(test_set,b_length)\n",
    "\n",
    "print(\"For training set head of frequency_day_vectors is:\")\n",
    "print(head(frequency_day_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OW and PW\n",
    "For the testing dataset (**test_frequency_day_vectors**) OW and PW are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"For OW:\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "1 2014-01-21   1   0   1   2   1   1   2   0   0    0    0\n",
      "2 2014-01-22   1   1   1   1   2   0   1   0   0    0    0\n",
      "3 2014-01-23   1   1   1   1   1   2   0   1   0    0    0\n",
      "4 2014-01-24   0   0   1   1   2   0   1   0   0    1    0\n",
      "5 2014-01-25   1   1   2   1   1   2   1   0   1    0    0\n",
      "6 2014-01-26   1   1   0   1   1   1   2   0   0    0    1\n",
      "[1] \"\"\n",
      "[1] \"the PW is:\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "6 2014-01-26   1   1   0   1   1   1   2   0   0    0    1\n",
      "7 2014-01-27   0   1   2   1   1   1   1   0   1    0    0\n",
      "8 2014-01-28   1   1   1   1   1   0   1   0   0    1    0\n",
      "[1] \"-----------------------------------------------------\"\n",
      "[1] \"For OW:\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "3 2014-01-23   1   1   1   1   1   2   0   1   0    0    0\n",
      "4 2014-01-24   0   0   1   1   2   0   1   0   0    1    0\n",
      "5 2014-01-25   1   1   2   1   1   2   1   0   1    0    0\n",
      "6 2014-01-26   1   1   0   1   1   1   2   0   0    0    1\n",
      "7 2014-01-27   0   1   2   1   1   1   1   0   1    0    0\n",
      "8 2014-01-28   1   1   1   1   1   0   1   0   0    1    0\n",
      "[1] \"\"\n",
      "[1] \"the PW is:\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "8  2014-01-28   1   1   1   1   1   0   1   0   0    1    0\n",
      "9  2014-01-29   0   0   1   2   0   0   2   0   0    0    0\n",
      "10 2014-01-30   1   1   2   1   2   2   0   0   0    0    1\n",
      "[1] \"-----------------------------------------------------\"\n"
     ]
    }
   ],
   "source": [
    " for(i in  seq(1,nrow(test_frequency_day_vectors), by=N)){\n",
    "      \n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(test_frequency_day_vectors)){\n",
    "      break\n",
    "    }\n",
    "    \n",
    "    #take a window of X*M=6 days, [1:6] [3:8] [5:10] ... [11 16]\n",
    "    #subset by row, get the X*M days\n",
    "    OW = test_frequency_day_vectors[i:((X*M)+(i-1)),] \n",
    "    #subset by row, get the Y days\n",
    "    PW = test_frequency_day_vectors[(i-1+X*M+Z):(i-1+X*M+Z+Y-1),]\n",
    "    \n",
    "    print(\"For OW:\")\n",
    "    print(OW)\n",
    "    print(\"\")\n",
    "    print(\"the PW is:\")\n",
    "    print(PW) \n",
    "    print(\"-----------------------------------------------------\")\n",
    "     \n",
    "}     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for creating new instances for the dataset\n",
    "Each created instance is comprised by five feature categories: \n",
    "\n",
    "- **Basic Features**: A frequency vector for each error type inside an OW. \n",
    "\n",
    "- **Advanced Statistical Features**: A vector of statistics like, minimum, maximum and mean distance of an error type inside the OW, from the beginning of the corresponding PW and mean and standard deviation of the distance between instances of the same error type inside the OW, for each error type. \n",
    "\n",
    "- **Pattern-based Features**: A binary vector of error type patterns, which is created based on a confidence threshold on the relative frequency of each pattern in all the OWs. The initial set of patterns is created based on the power set (excluding the null set) of the error types inside each OW. \n",
    "\n",
    "- **Failure Similarity Features**: The Jaccard similarity of two consecutive failures (tickets) of the same type, computed based onthe error types of each corresponding OW. \n",
    "\n",
    "- **Profile-based Features**: Consider equipment specific features, like the model and the installation date of a ATM machine.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**1) function: create_instances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates new instances(for a dataset) from frequency vector.\n",
    "\n",
    "create_instances <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,PATTERN_FEATURES,SIMILARITY_FEATURE,test,conf,sup){\n",
    "  \n",
    "  #frequency_day_vectors withoult column timestamps  \n",
    "  frequency_day_vectors = frequency_day_vectors[ , !(names(frequency_day_vectors) %in% c(\"Timestamps\"))]\n",
    "\n",
    "  #init data frame instances_df(function: init)  \n",
    "  instances_df = init(frequency_day_vectors,target_event,X,M,Y,Z,N,PATTERN_FEATURES,SIMILARITY_FEATURE,test,conf,sup);\n",
    "\n",
    "  #for each OW\n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "      \n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){\n",
    "      break\n",
    "    }\n",
    "    \n",
    "    #take a window of X*M=6 days, [1:6] [3:8] [5:10] ... [11 16]\n",
    "    #subset by row, get the X*M days\n",
    "    OW = frequency_day_vectors[i:((X*M)+(i-1)),]  \n",
    "\n",
    "    #compute the new instances of each OW   \n",
    "    instance = compute_pattern_features(PATTERN_FEATURES,OW)\n",
    "    instance = c(instance,compute_similarity_feature(SIMILARITY_FEATURE,OW))\n",
    "    instance = c(instance,compute_advanced_statistic_features(OW))\n",
    "    instance = compute_basic_statistic_features(instance,OW)\n",
    "      \n",
    "    #save it as a new data frame row \n",
    "    instance_df = as.data.frame(instance)\n",
    "\n",
    "    #check the label of the OW\n",
    "    Label = label_OW(i,frequency_day_vectors,target_event,X,M,Y,Z,N,test)\n",
    "    \n",
    "    #if a buffer window is used and target event is happened in it    \n",
    "    if(is.null(Label)){\n",
    "      if(!csv){\n",
    "        print(\"moving to the next\")\n",
    "      }\n",
    "      next\n",
    "    }\n",
    "    \n",
    "    #set label of instance_df to Label    \n",
    "    instance_df$Label = Label\n",
    "    \n",
    "    #set instance_df as a new row of instances_df data frame  \n",
    "    instances_df = rbind(instances_df,instance_df) \n",
    "  }\n",
    "    \n",
    "  return(instances_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) function: init**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that inits the instances data frame.\n",
    "\n",
    "init <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,PATTERN_FEATURES,SIMILARITY_FEATURE,test,conf,sup){\n",
    "  \n",
    "  #make a data frame named instances_df with ncol=(5+ #of sub-windows)*(# of frequency_day_vectors' columns)+1\n",
    "  #                                     and  nrow=0\n",
    "  instances_df = data.frame(matrix(ncol = (ncol(frequency_day_vectors)*5+ncol(frequency_day_vectors)*X+1), nrow = 0))\n",
    "  \n",
    "  #if we chose to get pattern features\n",
    "  #---------------------------------------------------------------------------------------------------  \n",
    "  if(PATTERN_FEATURES && SIMILARITY_FEATURE){\n",
    "    if(!test){\n",
    "      #get the frequent patterns(function: getFrequentPatterns)  \n",
    "      freq_pattern_items <<- getFrequentPatterns(frequency_day_vectors,target_event,X,M,Y,Z,N,conf,sup)\n",
    "    }\n",
    "    #increase the instances_df column size by the length of the patterns + 1(for similarity feature) \n",
    "    instances_df = data.frame(matrix(ncol = (ncol(frequency_day_vectors)*5+ncol(frequency_day_vectors)*X+length(freq_pattern_items)+1+1), nrow = 0))\n",
    "  } else if(PATTERN_FEATURES){\n",
    "    if(!test){\n",
    "      #get the frequent patterns(function: getFrequentPatterns)   \n",
    "      freq_pattern_items <<- getFrequentPatterns(frequency_day_vectors,target_event,X,M,Y,Z,N,conf,sup)\n",
    "    }\n",
    "    #increase the instances_df column size by the length of the patterns  \n",
    "    instances_df = data.frame(matrix(ncol = (ncol(frequency_day_vectors)*5+ncol(frequency_day_vectors)*X+length(freq_pattern_items)+1), nrow = 0))\n",
    "  #--------------------------------------------------------------------------------------------------- \n",
    "  \n",
    "  #if we did not choose to get pattern features, but we chose similarity feature\n",
    "  #---------------------------------------------------------------------------------------------------     \n",
    "  } else if(SIMILARITY_FEATURE){\n",
    "    #increase the instances_df column size by 1(for similarity feature)\n",
    "    instances_df = data.frame(matrix(ncol = (ncol(frequency_day_vectors)*5+ncol(frequency_day_vectors)*X+1+1), nrow = 0))\n",
    "  }\n",
    "  #---------------------------------------------------------------------------------------------------  \n",
    "  \n",
    "  #if it is not for a test dataset and we chose similarity feature    \n",
    "  if(!test && SIMILARITY_FEATURE){\n",
    "    #get the first positive Observed Window(function: get_first_positive_OW)  \n",
    "    first_positive_OW <<- get_first_positive_OW(frequency_day_vectors,target_event,X,M,Y,Z,N,test)\n",
    "    \n",
    "    print(\"First_positive OW is:\")\n",
    "    print(first_positive_OW)  \n",
    "    print(\"\")    \n",
    "  }\n",
    "  \n",
    "  if(PATTERN_FEATURES && length(freq_pattern_items) <= 0){\n",
    "    if(!csv){\n",
    "      print(\"WARNING: No patterns found.\")\n",
    "      print(\"\")\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  return(instances_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Functions for exporting patterns using frequency\n",
    "**3a) function: getFrequentPatterns**\n",
    "\n",
    "**3b) function: getFrequentPatternsPaperConf**\n",
    "\n",
    "**3c) function: getFrequentPatternsCustom**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used for export frequent patterns.\n",
    "\n",
    "#For calling the appropriate function \n",
    "getFrequentPatterns <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,conf_level,support){\n",
    "  if(PATTERN_CUSTOM){\n",
    "    return(getFrequentPatternsCustom(frequency_day_vectors,target_event,X,M,Y,Z,N,conf_level,support))\n",
    "  } else {\n",
    "    return(getFrequentPatternsPaperConf(frequency_day_vectors,target_event,X,M,Y,Z,N,conf_level,support))\n",
    "  }\n",
    "}\n",
    "\n",
    "#Two same Functions\n",
    "#----------------------------------------------------------------------------------------------------------\n",
    "#Function that returns the patterns using apriori algorithm and filtering them with a confidence threashold\n",
    "getFrequentPatternsPaperConf <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,conf_level,support){\n",
    "  test = FALSE\n",
    "  events_list <- list()\n",
    "  \n",
    "  #for each OW   \n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "    #if the end of Predictive Window is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){   #I CHANGED IT\n",
    "      break\n",
    "    }\n",
    "    OW = frequency_day_vectors[i:((X*M)+(i-1)),] #Observation Window = subset of X*M days\n",
    "    freq = sapply(OW, sum) #for each OW sum the frequences for every fault event\n",
    "    events = names(freq[freq>0]) #keep the names of the events with freq>0\n",
    "    events_list[length(events_list)+1] = list(events) #save the names to a list\n",
    "  }\n",
    "  names(events_list) <- paste(\"OW\",c(1:length(events_list)), sep = \"\") #name each value of the list as \"OW(i)\" 1<=i<=(#of OWs)\n",
    "  trans1 <- as(events_list, \"transactions\") #make list to transactions\n",
    "    \n",
    "  #https://www.rdocumentation.org/packages/arules/versions/1.6-4/topics/apriori  \n",
    "  freq_patterns <- apriori(trans1, parameter = list(supp=support,target=\"frequent\",maxtime=5,maxlen=10, minlen=1),control=list(verbose = FALSE))\n",
    "  \n",
    "  #if no frequency patterns found return   \n",
    "  if(length(items(freq_patterns)) == 0){\n",
    "    return(items(freq_patterns))\n",
    "  }\n",
    "  \n",
    "  #init integer vectors with length(items(freq_patterns)) zeros   \n",
    "  total_freq = integer(length(items(freq_patterns)))\n",
    "  positive_freq = integer(length(items(freq_patterns)))\n",
    "    \n",
    "  #for each OW  \n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){   #I CHANGED IT\n",
    "      break\n",
    "    }\n",
    "\n",
    "    #check the label of the OW(function: label_OW)\n",
    "    Label = label_OW(i,frequency_day_vectors,target_event,X,M,Y,Z,N,test)\n",
    "      \n",
    "    if(is.null(Label)){\n",
    "      if(!csv){\n",
    "        print(\"Error in BW moving to the next OW\")\n",
    "      }\n",
    "      next\n",
    "    }\n",
    "    \n",
    "    OW = frequency_day_vectors[i:((X*M)+(i-1)),] #Observation Window = subset of X*M days\n",
    "      \n",
    "    logical_v = items(freq_patterns) %in% colnames(OW[, colSums(OW != 0) > 0]) #\n",
    "      \n",
    "    for(i in 1:length(logical_v)){\n",
    "      if(logical_v[i]){\n",
    "        #if the OW is labeled(which means that inside OW's PW the target event happened)   \n",
    "        if(Label){\n",
    "          positive_freq[i] = positive_freq[i] + 1\n",
    "        } \n",
    "        total_freq[i] = total_freq[i] + 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  conf = positive_freq/total_freq #for each frequency pattern set conf=positive_freq/total_freq\n",
    "  \n",
    "  return(items(freq_patterns)[conf>=conf_level]) #return only the frequency patterns with minimum confidence=conf_level\n",
    "}\n",
    "\n",
    "\n",
    "getFrequentPatternsCustom <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,conf_level,support){\n",
    "  test = FALSE\n",
    "  events_list <- list()\n",
    "  \n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){\n",
    "      break\n",
    "    }\n",
    "    #check the label of the OW\n",
    "    Label = label_OW(i,frequency_day_vectors,target_event,X,M,Y,Z,N,test)\n",
    "    if(is.null(Label)){\n",
    "      if(!csv){\n",
    "        print(\"Error in BW moving to the next OW\")\n",
    "      }\n",
    "      next\n",
    "    }\n",
    "    \n",
    "    #only for the labeled OWs\n",
    "    if(Label){\n",
    "      OW = frequency_day_vectors[i:((X*M)+(i-1)),] #subset by row, get the X*M days\n",
    "      freq = sapply(OW, sum)\n",
    "      events = names(freq[freq>0])\n",
    "      events_list[length(events_list)+1] = list(events)\n",
    "    }\n",
    "  }\n",
    "  names(events_list) <- paste(\"OW\",c(1:length(events_list)), sep = \"\")\n",
    "  trans1 <- as(events_list, \"transactions\")\n",
    "  freq_patterns <- apriori(trans1, parameter = list(supp=support,target=\"frequent\",maxtime=5,maxlen=10, minlen=1),control=list(verbose = FALSE))\n",
    "  if(length(items(freq_patterns)) == 0){\n",
    "    return(items(freq_patterns))\n",
    "  }\n",
    "  total_freq = integer(length(items(freq_patterns)))\n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){\n",
    "      break\n",
    "    }\n",
    "    \n",
    "    OW = frequency_day_vectors[i:((X*M)+(i-1)),] #subset by row, get the X*M days\n",
    "    logical_v = items(freq_patterns) %in% colnames(OW[, colSums(OW != 0) > 0])\n",
    "    for(i in 1:length(logical_v)){\n",
    "      if(logical_v[i]){\n",
    "        total_freq[i] = total_freq[i] + 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  conf = quality(freq_patterns)$count/total_freq\n",
    "  \n",
    "  \n",
    "\n",
    "  return(items(freq_patterns)[conf>=conf_level])\n",
    "}\n",
    "#----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Functions for exporting Observation Windows' informations\n",
    "**4a) function: get_first_positive_OW**\n",
    "\n",
    "**4b) function: label_OW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used for exporting informations about the Observation Windows.\n",
    "\n",
    "#Function that returns the first positive OW, wich means that inside its PW the target event happened\n",
    "get_first_positive_OW <- function(frequency_day_vectors,target_event,X,M,Y,Z,N,test){\n",
    "   \n",
    "  #for every OW  \n",
    "  for(i in  seq(1,nrow(frequency_day_vectors), by=N)){\n",
    "    \n",
    "    #if the end of PW is equal to the total days then stop\n",
    "    if((i-1+X*M+Z+Y-1) > nrow(frequency_day_vectors)){\n",
    "      break\n",
    "    }\n",
    "\n",
    "    #check the label of the OW(function: label_OW)\n",
    "    Label = label_OW(i,frequency_day_vectors,target_event,X,M,Y,Z,N,test)\n",
    "\n",
    "    if(is.null(Label)){\n",
    "      if(!csv){\n",
    "        print(\"moving to the next\")\n",
    "      }\n",
    "      next\n",
    "    }\n",
    "    \n",
    "    if(Label){\n",
    "      #if you found a labeled OW, return it \n",
    "      return(frequency_day_vectors[i:((X*M)+(i-1)),]) #subset by row, get the X*M days\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "#Function that returns the label(boolean variable) of an OW\n",
    "#                         true->the target event happened in OW's PW\n",
    "#                         false->the target event did not happen in OW's PW\n",
    "label_OW <- function(index,frequency_day_vectors,target_event,X,M,Y,Z,N,test){\n",
    "  \n",
    "  PW = data.frame() #init an empty data frame PW   \n",
    "  \n",
    "  if(test){ #if it is for a test dataset  \n",
    "    PW = frequency_day_vectors[(index-1+X*M):(index-1+X*M+Z+Y-1),] #Predictive Window with length=Y     \n",
    "  } else { #if it is not for a test dataset     \n",
    "    if(Z != 0){ #if we use beffer window\n",
    "      BW = frequency_day_vectors[(index-1+((X*M))):(index-1+((X*M))+Z-1),] #buffer window with length=Z\n",
    "      if(sum(BW[paste(\"e_\",target_event,sep=\"\")]) > 0){\n",
    "        return(NULL) #return null if inside the BW target event happened\n",
    "      }\n",
    "    }\n",
    "    #if we do not use beffer window\n",
    "    PW = frequency_day_vectors[(index-1+X*M+Z):(index-1+X*M+Z+Y-1),]\n",
    "  }\n",
    "  return(sum(PW[paste(\"e_\",target_event,sep=\"\")]) > 0) #return true if inside the PW target event happened, else false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Functions for computing this new instances of the set\n",
    "**5a) function: compute_pattern_features**\n",
    "\n",
    "**5b) function: compute_similarity_feature**\n",
    "\n",
    "**5c) function: jaccard**\n",
    "\n",
    "**5d) function: compute_advanced_statistic_features**\n",
    "\n",
    "**5e) function: compute_basic_statistic_features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used to compute the new instances of the set.\n",
    "\n",
    "#Function to compute pattern features.\n",
    "compute_pattern_features <- function(PATTERN_FEATURES,OW){\n",
    "  P=list()\n",
    "  if(PATTERN_FEATURES && length(freq_pattern_items) > 0){\n",
    "    P = setNames(as.list(rep(0,length(freq_pattern_items))),as.list(paste(\"p_\",labels(freq_pattern_items),sep=\"\")))\n",
    "    logical_v = freq_pattern_items %in% colnames(OW[, colSums(OW != 0) > 0])\n",
    "    for(l in 1:length(logical_v)){\n",
    "      if(logical_v[l]){\n",
    "        P[l] = 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return(P)\n",
    "}\n",
    "\n",
    "\n",
    "#Function to compute similarity features, based in Jaccard Similarity.\n",
    "compute_similarity_feature <- function(SIMILARITY_FEATURE,OW){\n",
    "  J=list()\n",
    "  if(SIMILARITY_FEATURE){\n",
    "    J = setNames(list(0),list(\"jaccard\"))\n",
    "    J[1] = jaccard(data.frame(OW,first_positive_OW),2)#JACARD SIMILARITY BETWEEN OW-FIRST_POSITIVE_OW\n",
    "  }\n",
    "  return(J)\n",
    "}\n",
    "\n",
    "#Function the returns the Jaccard Distance\n",
    "jaccard <- function(df, margin) {\n",
    "  if (margin == 1 | margin == 2) {\n",
    "    M_00 <- apply(df, margin, sum) == 0\n",
    "    M_11 <- apply(df, margin, sum) == 2\n",
    "    if (margin == 1) {\n",
    "      df <- df[!M_00, ]\n",
    "      JSim <- sum(M_11) / nrow(df)\n",
    "    } else {\n",
    "      df <- df[, !M_00]\n",
    "      JSim <- sum(M_11) / length(df)\n",
    "    }\n",
    "    JDist <- 1 - JSim\n",
    "    return(JDist)\n",
    "  } else break\n",
    "}\n",
    "\n",
    "\n",
    "#Function to compute advanced statistical features for each OW's fault events as:\n",
    "\n",
    "#e_X_min_d: 1->every row of OW has in row e_X zero value\n",
    "#           0->at least one row of OW has non zero value at column e_X\n",
    "\n",
    "#e_X_max_d: max distance between a non zero value in column e_X and the end of OW\n",
    "\n",
    "#e_X_mean_d: (sum of all distances between non zero values in column e_X and the end of OW)/(# of the OW's rows)\n",
    "\n",
    "#e_X_mean_V:ex   e_X            (mean of all distances between two consecutively non zero values)\n",
    "#                 1-|DIST1=1  \n",
    "#                 1-|--------|DIST2=1    \n",
    "#                 1-|--------|---------|DIST3=1               e_X_mean_V = (DIST1+DIST2+DIST3+DIST4)/4 = 5/4 = 1.25\n",
    "#                 0                    |\n",
    "#                 1-|--------|---------|--------|DIST4=1\n",
    "#                 1-|--------|---------|--------|\n",
    "\n",
    "\n",
    "#e_X_std_V: sample standard diviation of the error interval and for the the same example:\n",
    "#           e_X_std_V=(((DIST1-e_X_mean_V)^2+(DIST2-e_X_mean_V)^2+(DIST3-e_X_mean_V)^2+(DIST4-e_X_mean_V)^2)/(4-1))^(1/2)\n",
    "\n",
    "compute_advanced_statistic_features <- function(OW){\n",
    "  #compute error interval\n",
    "  V <- setNames(as.list(rep(0,(2*length(names(OW))))), c(paste(names(OW),\"_meanV\",sep=\"\"), paste(names(OW),\"_stdV\",sep=\"\")))\n",
    "  for(e in 1:length(OW)){\n",
    "    event = OW[e]\n",
    "    error_interval = c()\n",
    "    event_indeces = which(event > 0)\n",
    "    for(d in length(event_indeces):2){\n",
    "      error_interval[length(error_interval)+1] = event_indeces[d] - event_indeces[d-1] \n",
    "    }\n",
    "    meanV = mean(error_interval)\n",
    "    meanV = if(is.na(meanV)) 0 else meanV\n",
    "    stdV = sd(error_interval)\n",
    "    stdV = if(is.na(stdV)) 0 else stdV\n",
    "    V[paste(names(event),\"_meanV\",sep=\"\")] = meanV\n",
    "    V[paste(names(event),\"_stdV\",sep=\"\")] = stdV\n",
    "  }\n",
    "  \n",
    "  #compute distance from Prediction Point\n",
    "  D <- data.frame(matrix(ncol = b_length, nrow = 0))\n",
    "  for(d in 1:nrow(OW)){\n",
    "    error_instance_distance = nrow(OW)-d+1\n",
    "    day = OW[d,]\n",
    "    day[1:ncol(day)][day[1:ncol(day)] > 0] = error_instance_distance\n",
    "    \n",
    "    D <- rbind(D,day)\n",
    "  }\n",
    "  min = sapply(D,min)\n",
    "  names(min) = paste(names(min),\"_minD\",sep=\"\")\n",
    "  \n",
    "  max = sapply(D,max)\n",
    "  names(max) = paste(names(max),\"_maxD\",sep=\"\")\n",
    "  \n",
    "  mean = sapply(D,mean)\n",
    "  names(mean) = paste(names(mean),\"_meanD\",sep=\"\")\n",
    "\n",
    "  return(c(min,max,mean,V))\n",
    "}\n",
    "\n",
    "#Function to compute basic statistical features for each OW's fault events as frequency:\n",
    "compute_basic_statistic_features <- function(instance,OW){\n",
    "  \n",
    "    OW = split(OW, factor(sort(rank(row.names(OW))%%X)))\n",
    "\n",
    "  for(j in 1:length(OW)){\n",
    "    SW = OW[j]\n",
    "    freq = sapply(SW[[1]], sum)     \n",
    "    names(freq) = paste(names(freq),paste(\"_freq_\",j,sep=\"\"),sep=\"\")  \n",
    "    instance = c(instance,freq)\n",
    "  }\n",
    "\n",
    "  return(instance)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create instances\n",
    "**For the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"First_positive OW is:\"\n",
      "   e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "5    1   1   1   1   1   2   2   0   0    0    0\n",
      "6    1   1   1   1   1   0   0   1   0    1    0\n",
      "7    0   1   2   0   1   1   2   0   1    0    0\n",
      "8    1   1   1   1   1   1   1   2   0    1    0\n",
      "9    1   1   0   2   1   0   0   0   0    0    0\n",
      "10   1   1   2   2   1   1   2   0   0    1    0\n",
      "[1] \"\"\n",
      "[1] \"WARNING: No patterns found.\"\n",
      "[1] \"\"\n",
      "[1] \"The form of the training instances_df looks like:\"\n",
      "    jaccard e_2_minD e_3_minD e_4_minD e_5_minD e_7_minD e_1_maxD e_2_maxD\n",
      "1 1.0000000        0        0        0        0        0        4        5\n",
      "2 0.9500000        0        1        0        1        0        6        6\n",
      "3 1.0000000        1        0        0        1        0        6        6\n",
      "4 0.9047619        0        0        0        1        0        5        6\n",
      "5 0.9000000        0        0        1        1        0        6        6\n",
      "6 0.9000000        0        0        1        0        0        6        5\n",
      "  e_3_maxD e_4_maxD e_5_maxD e_6_maxD e_7_maxD e_8_maxD e_9_maxD e_10_maxD\n",
      "1        5        5        5        5        5        4        0         1\n",
      "2        6        6        6        6        6        6        2         3\n",
      "3        6        6        6        6        6        5        4         5\n",
      "4        6        5        6        6        6        5        6         5\n",
      "5        5        6        6        5        5        2        0         5\n",
      "6        6        6        6        4        5        4        0         4\n",
      "  e_11_maxD e_1_meanD e_2_meanD e_3_meanD e_4_meanD e_5_meanD e_6_meanD\n",
      "1         6  1.166667  2.000000  2.500000  2.500000  2.500000  2.333333\n",
      "2         0  2.333333  2.666667  3.500000  3.166667  3.500000  3.000000\n",
      "3         0  2.833333  3.500000  3.166667  2.833333  3.500000  2.333333\n",
      "4         2  2.333333  3.166667  2.666667  2.500000  3.500000  2.333333\n",
      "5         4  3.000000  2.500000  2.000000  3.500000  3.500000  1.333333\n",
      "6         6  2.333333  1.500000  2.666667  3.500000  3.166667  1.666667\n",
      "  e_7_meanD e_8_meanD e_9_meanD e_10_meanD e_11_meanD e_1_meanV e_2_meanV\n",
      "1  1.833333 1.3333333 0.0000000  0.1666667  1.0000000  1.500000  1.333333\n",
      "2  2.166667 2.5000000 0.3333333  0.6666667  0.0000000  1.666667  1.250000\n",
      "3  2.333333 1.3333333 0.6666667  1.5000000  0.0000000  1.250000  1.000000\n",
      "4  2.500000 0.8333333 1.0000000  1.3333333  0.3333333  1.000000  1.250000\n",
      "5  1.833333 0.5000000 0.0000000  1.1666667  0.6666667  1.250000  1.666667\n",
      "6  2.500000 1.1666667 0.0000000  0.6666667  1.1666667  1.666667  2.000000\n",
      "  e_3_meanV e_4_meanV e_5_meanV e_6_meanV e_7_meanV e_8_meanV e_10_meanV\n",
      "1  1.000000      1.00      1.00  1.000000  1.500000  1.500000          0\n",
      "2  1.000000      1.25      1.00  1.250000  1.666667  1.666667          2\n",
      "3  1.250000      1.25      1.00  1.666667  1.666667  2.000000          2\n",
      "4  1.333333      1.00      1.00  1.500000  1.666667  0.000000          2\n",
      "5  1.333333      1.00      1.00  2.000000  1.333333  1.000000          3\n",
      "6  1.250000      1.00      1.25  1.000000  1.000000  1.000000          0\n",
      "  e_11_meanV  e_1_stdV  e_2_stdV  e_3_stdV e_4_stdV e_5_stdV  e_6_stdV\n",
      "1          0 0.7071068 0.5773503 0.0000000      0.0      0.0 0.0000000\n",
      "2          0 0.5773503 0.5000000 0.0000000      0.5      0.0 0.5000000\n",
      "3          0 0.5000000 0.0000000 0.5000000      0.5      0.0 0.5773503\n",
      "4          0 0.0000000 0.5000000 0.5773503      0.0      0.0 0.7071068\n",
      "5          0 0.5000000 0.5773503 0.5773503      0.0      0.0 1.4142136\n",
      "6          5 0.5773503 0.0000000 0.5000000      0.0      0.5 0.0000000\n",
      "   e_7_stdV  e_8_stdV e_1_freq_1 e_2_freq_1 e_3_freq_1 e_4_freq_1 e_5_freq_1\n",
      "1 0.7071068 0.7071068          0          1          1          1          1\n",
      "2 0.5773503 0.5773503          1          1          2          2          3\n",
      "3 0.5773503 0.0000000          2          2          2          2          2\n",
      "4 0.5773503 0.0000000          1          2          3          1          2\n",
      "5 0.5773503 0.0000000          2          2          2          4          2\n",
      "6 0.0000000 0.0000000          1          1          2          2          4\n",
      "  e_6_freq_1 e_7_freq_1 e_8_freq_1 e_9_freq_1 e_10_freq_1 e_11_freq_1\n",
      "1          1          1          0          0           0           1\n",
      "2          3          1          3          0           0           0\n",
      "3          2          2          1          0           1           0\n",
      "4          2          3          2          1           1           0\n",
      "5          1          2          0          0           1           0\n",
      "6          0          2          0          0           0           1\n",
      "  e_1_freq_2 e_2_freq_2 e_3_freq_2 e_4_freq_2 e_5_freq_2 e_6_freq_2 e_7_freq_2\n",
      "1          1          1          2          2          3          3          1\n",
      "2          2          2          2          2          2          2          2\n",
      "3          1          2          3          1          2          2          3\n",
      "4          2          2          2          4          2          1          2\n",
      "5          1          1          2          2          4          0          2\n",
      "6          2          1          2          2          2          2          3\n",
      "  e_8_freq_2 e_9_freq_2 e_10_freq_2 e_11_freq_2 e_1_freq_3 e_2_freq_3\n",
      "1          3          0           0           0          2          2\n",
      "2          1          0           1           0          1          2\n",
      "3          2          1           1           0          2          2\n",
      "4          0          0           1           0          1          1\n",
      "5          0          0           0           1          2          1\n",
      "6          2          0           1           0          1          1\n",
      "  e_3_freq_3 e_4_freq_3 e_5_freq_3 e_6_freq_3 e_7_freq_3 e_8_freq_3 e_9_freq_3\n",
      "1          2          2          2          2          2          1          0\n",
      "2          3          1          2          2          3          2          1\n",
      "3          2          4          2          1          2          0          0\n",
      "4          2          2          4          0          2          0          0\n",
      "5          2          2          2          2          3          2          0\n",
      "6          2          2          2          2          3          0          0\n",
      "  e_10_freq_3 e_11_freq_3 Label\n",
      "1           1           0 FALSE\n",
      "2           1           0 FALSE\n",
      "3           1           0  TRUE\n",
      "4           0           1 FALSE\n",
      "5           1           0  TRUE\n",
      "6           0           1  TRUE\n"
     ]
    }
   ],
   "source": [
    "#Create instances for training set.\n",
    "\n",
    "instances_df = create_instances(frequency_day_vectors,target_event,X,M,Y,Z,N,PATTERN_FEATURES,JACCARD_FEATURE,FALSE,conf,sup)\n",
    "\n",
    "\n",
    "#remove columns with all values equal to zero\n",
    "instances_df = instances_df[, colSums(instances_df != 0) > 0]\n",
    "\n",
    "\n",
    "#what is factor -> https://www.stat.berkeley.edu/~s133/factors.html\n",
    "label = instances_df$Label\n",
    "instances_df$Label = as.factor(label)\n",
    "\n",
    "print(\"The form of the training instances_df (which also used for fitting the predicted model) looks like:\")\n",
    "print(head(instances_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the testing set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"WARNING: No patterns found.\"\n",
      "[1] \"\"\n",
      "[1] \"The form of the training test_instances_df looks like:\"\n",
      "    jaccard e_1_minD e_2_minD e_3_minD e_4_minD e_5_minD e_6_minD e_7_minD\n",
      "1 1.0000000        0        0        0        1        1        0        0\n",
      "2 0.9047619        0        0        0        1        1        0        0\n",
      "  e_8_minD e_9_minD e_10_minD e_11_minD e_1_maxD e_2_maxD e_3_maxD e_4_maxD\n",
      "1        0        0         0         0        6        5        6        6\n",
      "2        0        0         0         0        6        6        6        6\n",
      "  e_5_maxD e_6_maxD e_7_maxD e_8_maxD e_9_maxD e_10_maxD e_11_maxD e_1_meanD\n",
      "1        6        6        6        4        2         3         1  3.000000\n",
      "2        6        6        5        6        4         5         3  2.333333\n",
      "  e_2_meanD e_3_meanD e_4_meanD e_5_meanD e_6_meanD e_7_meanD e_8_meanD\n",
      "1  2.000000  3.333333       3.5       3.5  2.166667  2.833333 0.6666667\n",
      "2  2.666667  3.000000       3.5       3.5  2.500000  2.500000 1.0000000\n",
      "  e_9_meanD e_10_meanD e_11_meanD e_1_meanV e_2_meanV e_3_meanV e_4_meanV\n",
      "1 0.3333333        0.5  0.1666667  1.250000  1.333333      1.00         1\n",
      "2 1.0000000        1.0  0.5000000  1.666667  1.250000      1.25         1\n",
      "  e_5_meanV e_6_meanV e_7_meanV e_8_meanV e_9_meanV e_10_meanV e_11_meanV\n",
      "1         1  1.666667      1.25         0         0          0          0\n",
      "2         1  1.333333      1.00         0         2          4          0\n",
      "   e_1_stdV  e_2_stdV e_3_stdV e_4_stdV e_5_stdV  e_6_stdV e_7_stdV e_8_stdV\n",
      "1 0.5000000 0.5773503      0.0        0        0 0.5773503      0.5        0\n",
      "2 0.5773503 0.5000000      0.5        0        0 0.5773503      0.0        0\n",
      "  e_9_stdV e_10_stdV e_11_stdV e_1_freq_1 e_2_freq_1 e_3_freq_1 e_4_freq_1\n",
      "1        0         0         0          2          1          2          3\n",
      "2        0         0         0          1          1          2          2\n",
      "  e_5_freq_1 e_6_freq_1 e_7_freq_1 e_8_freq_1 e_9_freq_1 e_10_freq_1\n",
      "1          3          1          3          0          0           0\n",
      "2          3          2          1          1          0           1\n",
      "  e_11_freq_1 e_1_freq_2 e_2_freq_2 e_3_freq_2 e_4_freq_2 e_5_freq_2 e_6_freq_2\n",
      "1           0          1          1          2          2          3          2\n",
      "2           0          2          2          2          2          2          3\n",
      "  e_7_freq_2 e_8_freq_2 e_9_freq_2 e_10_freq_2 e_11_freq_2 e_1_freq_3\n",
      "1          1          1          0           1           0          2\n",
      "2          3          0          1           0           1          1\n",
      "  e_2_freq_3 e_3_freq_3 e_4_freq_3 e_5_freq_3 e_6_freq_3 e_7_freq_3 e_8_freq_3\n",
      "1          2          2          2          2          3          3          0\n",
      "2          2          3          2          2          1          2          0\n",
      "  e_9_freq_3 e_10_freq_3 e_11_freq_3 Label\n",
      "1          1           0           1  TRUE\n",
      "2          1           1           0  TRUE\n"
     ]
    }
   ],
   "source": [
    "#Create instances for testing set.\n",
    "\n",
    "test_instances_df = create_instances(test_frequency_day_vectors,target_event,X,M,Y,Z,N,PATTERN_FEATURES,JACCARD_FEATURE,TRUE,conf,sup)\n",
    "\n",
    "#Convert column Label into a factor column.\n",
    "test_instances_df$Label = as.factor(test_instances_df$Label)\n",
    "\n",
    "print(\"The form of the training test_instances_df (which also used for the prediction) looks like:\")\n",
    "print(head(test_instances_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect target event's days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The failure_incidents are:\"\n",
      "[1]  6 10\n"
     ]
    }
   ],
   "source": [
    "#Find the rows of test_frequency_day_vectors where the target event happened.\n",
    "\n",
    "failure_incidents = which(matrix(grepl(1, test_frequency_day_vectors[,paste(\"e_\",target_event,sep=\"\")]),ncol=1),arr.ind=TRUE)[,1]#in which day-line the target event happened\n",
    "\n",
    "print(\"The failure_incidents are:\")\n",
    "print(failure_incidents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Feature Selection\n",
    "**1) function: feature_selection**\n",
    "\n",
    "**2) function: evalXGBoost (the 1st of XGBoost's functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns the minimum number of attributes of a data frame, using reliefF feature selection and \n",
    "#xgBoost predictive algorithm for evaluation, with the most information. \n",
    "\n",
    "feature_selection <- function(instances_df,test_instances_df,step,failure_incidents){\n",
    "    \n",
    "  run = TRUE\n",
    "  i = length(instances_df)-1 #-1 for taking out the label\n",
    "  max_F1=0 #variable for keeping the max_F1 score\n",
    "  max_instances_df = data.frame() #empty data frame named max_instances_df\n",
    "\n",
    "  while(run){\n",
    "    #Feature selection using reliefF\n",
    "    \n",
    "    #attrEval function -> https://www.rdocumentation.org/packages/CORElearn/versions/1.53.1/topics/attrEval  \n",
    "    #ReliefF -> https://medium.com/@yashdagli98/feature-selection-using-relief-algorithms-with-python-example-3c2006e18f83  \n",
    "    estReliefF <- attrEval(Label ~ ., instances_df, estimator=\"ReliefFexpRank\", ReliefIterations=50)\n",
    "    \n",
    "    #sort indeces of  estReliefF \n",
    "    sorted_indeces = order(estReliefF, decreasing = TRUE)\n",
    "    \n",
    "    #keep the the top i \"useful\" columns of instances data frame  \n",
    "    instances_df = instances_df %>% select(sorted_indeces[1:i],ncol(instances_df))\n",
    "    \n",
    "    #find F1 score using XGBoost(function: evalXGBoost)\n",
    "    F1 = evalXGBoost(instances_df,test_instances_df,failure_incidents,TRUE,FALSE)\n",
    "    \n",
    "    #if max F1 score is 0(first iteration)  \n",
    "    if(max_F1 == 0){\n",
    "      max_F1 = F1\n",
    "      max_instances_df = instances_df\n",
    "    } else if(F1>=max_F1){ #if F1>=max_F1(which means that with less data we have at least the same F1 score)\n",
    "      max_F1 = F1 #set as new max F1 score the current F1 score\n",
    "      max_instances_df = instances_df #set new max_instances_df the current instances_df\n",
    "    }\n",
    "    i = i - step #-step for taking out the least \"useful\" columns\n",
    "    if(i <= 0){\n",
    "      run = FALSE\n",
    "    }\n",
    "  }\n",
    "  return(max_instances_df)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for XGBoost \n",
    "**1) function: evalXGBoost**\n",
    "\n",
    "**2) function: eval_predictions**\n",
    "\n",
    "**3) function:compute_last_time_point_of_OW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions using for predicting and evaluating.\n",
    "\n",
    "#Function that uses XGBoost for predictive the target event(i.e. fault) and evaluating the results.\n",
    "\n",
    "evalXGBoost <- function(instances_df,test_instances_df,failure_incidents,fs=FALSE,plotbool=TRUE){\n",
    "    \n",
    "  set.seed(500) #for remaining the random output the same  \n",
    "  instances_df = instances_df[ , order(names(instances_df))] #order attributes by their names\n",
    " \n",
    "  #Training with XGBoost model using instances_df(made from training set) \n",
    "  #XGBoost tutorial and parameters explanation -> https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/  \n",
    "  #XGBoost tree construction example step by step -> https://www.youtube.com/watch?v=3CC4N4z3GJc\n",
    "  dtrain <- xgb.DMatrix(data = as.matrix(instances_df[ , !(names(instances_df) %in% c(\"Label\"))]),label = as.integer(as.logical(instances_df$Label)))\n",
    "  my.rf <- xgb.train(data = dtrain, nthread = 2, eta=0.6, nrounds = 10, objective = \"binary:logistic\",verbose = 2)\n",
    "  \n",
    "  #if plootbool then print the tree informations  \n",
    "  if(plotbool==TRUE)\n",
    "  {\n",
    "   print(\"------------------------------------------------------------------------------------------------------\")\n",
    "      tryCatch(\n",
    "        expr = {\n",
    "            print(xgb.plot.tree(model = my.rf))\n",
    "        },\n",
    "        error = function(e){ \n",
    "            print(xgb.dump(my.rf, with_stats = T))\n",
    "        }\n",
    "      )  \n",
    "    \n",
    "  } \n",
    "      \n",
    "  test_instances_df = test_instances_df[ , (names(test_instances_df) %in% names(instances_df))] #keep the same attributes as instances_df\n",
    "  test_instances_df = test_instances_df[ , order(names(test_instances_df))] #order the attributes by their name\n",
    "\n",
    "  #Predict test_instances_df's(made from testing set) regression value(using trees and the logistic function)\n",
    "  dtest <- xgb.DMatrix(data = as.matrix(test_instances_df[ , !(names(test_instances_df) %in% c(\"Label\"))]), label=as.integer(as.logical(test_instances_df$Label)))\n",
    "  Prediction <- predict(my.rf, dtest)\n",
    "  \n",
    "  if(plotbool){  \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Prediction  of test_instances_df is:\")\n",
    "    print(Prediction)\n",
    "  } \n",
    "    \n",
    "  #Use threashold 0.5 for classifing to the \"TRUE\" class, else to the \"FALSE\" class  \n",
    "  Prediction <- as.logical(as.numeric(Prediction > 0.5))  \n",
    "  \n",
    "  if(plotbool){  \n",
    "    print(\"Class prediction of test_instances_df is:\")\n",
    "    print(Prediction)  \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  }   \n",
    "      \n",
    "  #return F1 score of the prediction(function: eval_predictions)  \n",
    "  return(eval_predictions(Prediction,failure_incidents,fs))\n",
    "}\n",
    "\n",
    "#Function that evaluates the predictions of the XGBoost and retutrns F1 score.\n",
    "eval_predictions <- function(Prediction,failure_incidents,fs=FALSE){\n",
    "  predictions = list()\n",
    "   \n",
    "  #for every prediction  \n",
    "  for(p in 1:length(Prediction)){\n",
    "      \n",
    "    #compute last row of the OW(function: compute_last_time_point_of_OW)  \n",
    "    d = compute_last_time_point_of_OW(p)#d=X*M=3*2=6 8 10 ....\n",
    "\n",
    "    #if prediction is \"TRUE\", add last last day of OW at predictions  \n",
    "    if(Prediction[p] == \"TRUE\"){\n",
    "      predictions = c(predictions,d)\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  true_positives = 0\n",
    "  false_positives = 0\n",
    "  false_negatives = 0\n",
    "    \n",
    "  #for every row of the test_frequency_day_vectors where the target event happened(THE NUMBER OF PREDICTIONS DEPENDS ON THE NUMBER OF FAILURES)\n",
    "  for(i in 1:length(failure_incidents)){\n",
    "      \n",
    "    d = failure_incidents[i] #d = the current failure incident\n",
    "      \n",
    "    warnings = list() #empty list keeping the warnings\n",
    "      \n",
    "    if(i == 1){\n",
    "      #set warnings as the predictions before the current failure incident\n",
    "      warnings = predictions[predictions <= d] \n",
    "    } else {\n",
    "      #set warnings as the predictions before the current failure incident but after the previous failure incidence  \n",
    "      warnings = predictions[predictions > failure_incidents[i-1] & predictions <= d] \n",
    "    }\n",
    "    \n",
    "    #if there is no warning\n",
    "    if(length(warnings) == 0){\n",
    "      false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "    #if there is warning(s)    \n",
    "    } else {\n",
    "      #if there is warnings before the max interval from the failure(target event)  \n",
    "      if(length(warnings[warnings < d-max_warning_interval]) > 0){\n",
    "        #increase false positives by the number of these warnings  \n",
    "        false_positives = false_positives + length(warnings[warnings < d-max_warning_interval]) \n",
    "      }\n",
    "        \n",
    "      #if there is warnings after the max and before the min interval from the failure(target event)    \n",
    "      if(length(warnings[warnings >= (d-max_warning_interval)]) > 0 & length(warnings[warnings <= (d-min_warning_interval)]) > 0){\n",
    "        true_positives = true_positives + 1 #increase true positives by 1\n",
    "      #if there is no correct warning    \n",
    "      } else {\n",
    "        false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "    \n",
    "  precision = true_positives/(true_positives+false_positives) #calculate the precision of the model\n",
    "  \n",
    "  if((true_positives+false_positives) == 0){\n",
    "    precision = 0\n",
    "  }\n",
    "  \n",
    "  recall = true_positives/length(failure_incidents) #calculate recall of the model\n",
    "  \n",
    "  F1 = 2*((precision*recall)/(precision+recall)) #calculate F1 score of the model\n",
    "  if((precision+recall) == 0){\n",
    "    F1 = 0\n",
    "  }\n",
    "  \n",
    "  #prints  \n",
    "  if(!fs){\n",
    "    if(!csv){\n",
    "      cat(paste(\"dataset:\",argv$test,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\", false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\", precision,\"\\nrecall:\", recall,\"\\nF1:\", F1,\"\\n\"))\n",
    "    } else{\n",
    "      cat(paste(argv$test,\",\", true_positives,\",\", false_positives,\",\", false_negatives,\",\", precision,\",\", recall,\",\", F1,\",\",argv$fet,\",\",argv$tet,\",\",argv$X,\",\",argv$M,\",\",argv$Y,\",\",argv$Z,\",\",argv$N,\",\",argv$step,\",\",argv$sup,\",\",argv$conf,\",\",argv$fs,\",\",argv$pf,\",\",argv$sf,\",\",argv$plogic,\",\",argv$minwint,\",\",argv$maxwint, \"\\n\",sep=\"\"))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return(F1)\n",
    "}\n",
    "\n",
    "#Function that returns the last row of the OW(function: compute_last_time_point_of_OW)  \n",
    "compute_last_time_point_of_OW <- function(index){\n",
    "  OW_length = X*M\n",
    "  return(OW_length+((index-1)*N))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"Number of attributes before feature selection:\"\n",
      "[1] 5\n",
      "[1] \"\"\n",
      "[1] \"Number of attributes after feature selection:\"\n",
      "[1] 5\n",
      "[1] \"\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      " [1] \"booster[0]\"                           \n",
      " [2] \"0:leaf=0.109090917,cover=1.75\"        \n",
      " [3] \"booster[1]\"                           \n",
      " [4] \"0:leaf=0.0676070452,cover=1.74480367\" \n",
      " [5] \"booster[2]\"                           \n",
      " [6] \"0:leaf=0.0420069546,cover=1.73641109\" \n",
      " [7] \"booster[3]\"                           \n",
      " [8] \"0:leaf=0.0261137988,cover=1.72923923\" \n",
      " [9] \"booster[4]\"                           \n",
      "[10] \"0:leaf=0.0162320118,cover=1.72403765\" \n",
      "[11] \"booster[5]\"                           \n",
      "[12] \"0:leaf=0.010087356,cover=1.72052085\"  \n",
      "[13] \"booster[6]\"                           \n",
      "[14] \"0:leaf=0.0062675043,cover=1.71822679\" \n",
      "[15] \"booster[7]\"                           \n",
      "[16] \"0:leaf=0.0038936045,cover=1.71675992\" \n",
      "[17] \"booster[8]\"                           \n",
      "[18] \"0:leaf=0.00241852202,cover=1.71583247\"\n",
      "[19] \"booster[9]\"                           \n",
      "[20] \"0:leaf=0.00150213577,cover=1.71525025\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"Prediction  of test_instances_df is:\"\n",
      "[1] 0.5708255 0.5708255\n",
      "[1] \"Class prediction of test_instances_df is:\"\n",
      "[1] TRUE TRUE\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "dataset: C:/Users/Public/ptyxiakh/testing_my_dataset.csv \n",
      "true_positives: 1 \n",
      "false_positives: 0 \n",
      "false_negatives: 1 \n",
      "precision: 1 \n",
      "recall: 0.5 \n",
      "F1: 0.666666666666667 \n"
     ]
    }
   ],
   "source": [
    "#Run XGBoost algorithm and print the results.\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  length_before=(length(instances_df))\n",
    "  instances_df = feature_selection(instances_df,test_instances_df,step,failure_incidents)\n",
    "  length_after=(length(instances_df))\n",
    "  print(\"Number of attributes before feature selection:\")\n",
    "  print(length_before)\n",
    "  print(\"\")\n",
    "  print(\"Number of attributes after feature selection:\")\n",
    "  print(length_after)\n",
    "  print(\"\")\n",
    "    \n",
    "}\n",
    "\n",
    "resultXGBOOST = evalXGBoost(instances_df,test_instances_df,failure_incidents,FALSE,TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Random Forest \n",
    "**1) function: eval**\n",
    "\n",
    "**2) function: eval_predictions (the 2nd of XGBoost's functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that uses Random Forest for predictive the target event(i.e. fault) and evaluating the results.\n",
    "\n",
    "eval <- function(instances_df,test_instances_df,failure_incidents,plot){\n",
    "  set.seed(500) #for remaining the random output the same\n",
    "     \n",
    "  #Training with randomForest model using instances_df(made from training set)\n",
    "  #Random Forest R documentation and parameters explanation -> https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest  \n",
    "  #Random Forest idea -> https://www.youtube.com/watch?v=loNcrMjYh64\n",
    "  my.rf <-randomForest(Label ~ .,data=instances_df,importance=TRUE,ntree=2) #(default ntree=500) \n",
    "  \n",
    "  if(plot){  \n",
    "    varImpPlot(my.rf)  \n",
    "  }\n",
    "  \n",
    "  #Predict test_instances_df's(made from testing set)  \n",
    "  Prediction <- predict(my.rf, test_instances_df[ , !(names(test_instances_df) %in% c(\"Label\"))])\n",
    "  \n",
    "  #if ploot then print the tree informations  \n",
    "  if(plot)\n",
    "  {\n",
    "      print(\"------------------------------------------------------------------------------------------------------\")\n",
    "      print(getTree(my.rf, 1,labelVar=TRUE))\n",
    "      print(getTree(my.rf, 2,labelVar=TRUE))  \n",
    "      print(\"------------------------------------------------------------------------------------------------------\")\n",
    "      print(\"Prediction in eval is:\\n\")\n",
    "      print(Prediction)\n",
    "      print(\"------------------------------------------------------------------------------------------------------\")\n",
    "  }\n",
    "  \n",
    "  #return F1 score of the prediction(function: eval_predictions) \n",
    "  return(eval_predictions(Prediction,failure_incidents))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "  left daughter right daughter  split var split point status prediction\n",
      "1             2              3   e_4_maxD         5.5      1       <NA>\n",
      "2             0              0       <NA>         0.0     -1      FALSE\n",
      "3             4              5 e_7_freq_1         1.5      1       <NA>\n",
      "4             0              0       <NA>         0.0     -1      FALSE\n",
      "5             0              0       <NA>         0.0     -1       TRUE\n",
      "  left daughter right daughter  split var split point status prediction\n",
      "1             2              3 e_7_freq_1         1.5      1       <NA>\n",
      "2             0              0       <NA>         0.0     -1      FALSE\n",
      "3             0              0       <NA>         0.0     -1       TRUE\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"Prediction in eval is:\\n\"\n",
      "    1     2 \n",
      " TRUE FALSE \n",
      "Levels: FALSE TRUE\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "dataset: C:/Users/Public/ptyxiakh/testing_my_dataset.csv \n",
      "true_positives: 0 \n",
      "false_positives: 0 \n",
      "false_negatives: 2 \n",
      "precision: 0 \n",
      "recall: 0 \n",
      "F1: 0 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM\njIyampqnp6eysrK9vb2+vr7Hx8fQ0NDZ2dnh4eHp6enw8PD////ojgWfAAAACXBIWXMAABJ0\nAAASdAHeZh94AAAgAElEQVR4nO2diVbiUBQEH4vAgAL//7VDAkrYSWwlZarOGUchCd25FIRF\nLFsR+Tbl1QFE/gKKJBJAkUQCKJJIAEUSCaBIIgEUSSSAIokEUCSRAIokEkCRRAIokkgARRIJ\noEgiARRJJIAiiQRQJJEAiiQSQJFEAiiSSABFEgmgSCIBFEkkgCKJBFAkkQCKJBJAkahs3koZ\nrV+dQg4oEpVp2fHqEPKJo6BSyserI8gRRaLi3VGvcBq9YWfGclzGq+12MSrj5e6UdRnX52zK\nqLnYZlzq4zoP7XqEo+gNpcxrOd5n+/92J41L/WzCssyai+0smilSz3AUvaGU0e7e6K2M9v9N\ndifNS3XHtJ2UVXOx8Wa78dCuZziN3rC/E9qU2ppN7cm61unUmXKwSpF6hdPoDQczTv4bV19X\ne52+Fts0l5N+4DR6wzWR5qU+zFtcLqZI/cJp9IZrIq3LW/Xt+nIxReoXTqM3XBOpet7uo/nk\ntyL1FKfRG66KtCyzj+aT34rUU5xGb7gq0qaUkye/FamnOI3ecFWk7eT8ZVdF6iVOozdcF2lZ\nyvTa+YrUL5xGb7gu0u7YbnntfEXqF06j52xOn/yWnqJIPWf5eWQnvUaR+s1m7O/vIVCkXlNK\n9dYG6T+K1GvGpy/GSm9RJJEAiiQSQJFEAiiSSABFEgmgSCIBFEkkgCKJBFAkkQCKJBJAkUQC\nKJJIAEUSCaBIIgEUSSSAIokEUCSRAIokEkCRRAIokkgARRIJoEgiARRJJIAiiQRQJJEAiiQS\nQJFEAiiSSABFEgmgSCIBFEkkgCKJBFAkkQCKJBJAkUQCKJJIAEUSCaBIIgEUSSSAIokEUCSR\nAIokEkCRRAIokkgARRIJoEgiARRJJIAiiQRQJJEAiiQSQJFEAiiSSABFEgmgSCIBFEkkgCKJ\nBFAkkQCKJBJAkUQCKJJIAEUSCaBIIgEUSSSAIokEUCSRAIokEkCRRAIokkgARRIJoEgiARRJ\nJIAiiQRQJJEAiiQSQJFEAiiSSABFEgmgSCIBFEkkgCKJBFAkkQCKJBJAkUQCKJJIAEUSCaBI\nIgEUSSSAIokEUCSRAIokEkCRRAIokkgARRIJoEgiARRJJEBfRSpD5NU7vR2v3luv4ebe+M1d\n34K+5vpJYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADW\nGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBY\nZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABg\nnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGA\ndYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA\n1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIA\nWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgA\nYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIB\ngHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoE\nANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygS\nAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBI\nAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4Mi\nAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyK\nBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMo\nEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2g\nSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeD\nIgGAdYbFzZASqXxyevLmrZRZm+0sHl3uEKcU6vxbMxriiOIijU5Pnu5OmrfYzMf5lL+b608Q\nFumnZzTEEYUP7Vbl/WwrZd1m/Y+RIn1xvOuIdv75GQ1nRM/MqMve2IymFxfUZv1FmSjSgXpH\nHPZGsvMvzGgoI3puRnf2xmJcRotrZ0zL5uyC6iPyUjbjMj1dcTbaHU6cT2R3qK5IB8rF11a8\nckZDGdFzM7q9N6b1rp9cnvFx/pj1a0jT+uFsY8VJ9d38fCIfT9w8DmRKJ0cM7Tu/dEYDGdGT\nM7p5xqpMNtvNpKwuzjm/sdt+3flNNqcrLsvo4/rB9mOR/m23//7+v68hVT+3vma+dkYDGVFj\nRruf24u0H8WmnB9q727s3q7v87J/eNtYcVqPeNVJpAfn/xG+d4/02hkNZETfvUe68WrE7pD6\nyg3gycOx44pfp1xf4R7DmlLHx0ivndFQRvTNx0g3hzS6sooideVbz9q9dkZDGdE3n7W7tRs/\nLg8kzod0vg1Fusc3Xkd67YyGM6JvvY40vXJ0ULEoV55ubQ6pseL+23dFeo4Oj5FeOaMhjqiD\nSPWzObuRnN+2TcvHla00htRYcfWNZ+0enP8Xad35tTMa4oi6vI402b9h6/x9JePLJ1ZPh9Rc\ncVa/WqFIz9G+80tnNMQRdX1nQ3m7eH/W1d17MqTmiovqdXRFeo4OnV85oyGO6LW/j9TuXV6H\ndfIxes8rO3eY0RBHpEgEFKn/fEekUhovOzw49cYG2iz9dK4/xzc6v2JGQxyRIhFQpP7z0kO7\nLvQ1108C6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDO\nsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6\nw+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDr\nDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCs\nMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACw\nzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDA\nOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA\n6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkA\nrDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQA\nsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEA\nwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUC\nAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJ\nAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAk\nALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GR\nAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5shKtJ7p11YasazzXNLd7kIOMnOvzCjIY4o\nKtJm9I0hlTJaP7V0l4uAE+z8GzMa4oiiIk1LxyFVX9eTMnlq6S4X8ShAz0cfjPcbM/rBvdnf\nUQVFWnZs+bnWuKyeWbrLRTy8+N7OpyYX7ldm9GP7ss+j6iLSYlxGi4tT12Vy0XF3wryM5tvt\nrJRZdcJqd4s4qr6blPdtdcD+dtwzq+qH7oG7Un5ms0k6hHvljH5OpJ/d/LfoINK0Plq+uI+f\nlPWVIc2rZVeT6utuNvP9kfasGulod/5otDkOaVPG3wnckXL2fx9pn+2lM/qpXdnrUbUXaVUm\nm+1mcn4fPy/Ly3vdUi27OHwdVT8v68OL3VmLMq/XadxXP3WnXf5tt/+C/8oPbfcnMj7Na2f0\nU7uy16NqL9K0VM+Bbsr05NSP6ucrQ3qvv65PRrD/blIW+220FOmJZdrQ65u5A62zvXZG3iM9\nd8YnJ6eOmwcAjWXPv65X88Nh+u4gox7ei0Xq9YH3gdbhXjsjHyM9d8a1Ib3VRxGPhzRprDrb\nP7b9Wmv9mudW+/xU0IGMSL82I5+1e+6Ma+eU67eB50N6K+PFan3j1m55GFrHwN3p74sTBzqI\ndPXEX5rRD+7N/o6qy2OkKy8lPDmk+rvDkKa74+/J8ezqNYr37wT+w3R4jPTKGQ1xRB1EWpbR\nR/WEzvTyrIeHDdUD24/98Xd12zYvi6+zX/jOht7TuvNrZzTEEXV5HWly811XD4c0O9wkvm83\no/o1iurAwffaPaJ955fOaIgj6vrOhvJ2bY8+fiD7VsrkfbW7pXw7vGo++RzSZP7NwH+YDp1f\nOaMhjsjfRyIA6wyLm0GRAMA6w+Jm+I5Ix6eBysNTYwxxSt/o/IoZDXFEikRAkfqPh3YAYJ1h\ncTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWG\nxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZ\nFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhn\nWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCd\nYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1\nhsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADW\nGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBY\nZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABg\nnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGA\ndYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA\n1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIA\nWGdY3AyKBADWGRY3gyIBgHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgA\nYJ1hcTMoEgBYZ1jcDIoEANYZFjeDIgGAdYbFzaBIAGCdYXEzKBIAWGdY3AyKBADWGRY3gyIB\ngHWGxc2gSABgnWFxMygSAFhnWNwMigQA1hkWN4MiAYB1hsXNoEgAYJ1hcTMoEgBYZ1jcDIoE\nANYZFjeDIgGAdYbFzaBIAGCdYXEzREV6f7RSKWc/VoxnmxaXMcQpJTv/woyGOKKoSJtRpyGV\nMlo/fyFDnFKw82/MaIgjioo0LS1X2i+/npRJi3W+1h3OvIJNf2NGX5fgjO6dcZNl6/32ufy4\nrJ5f57jmYMaU6/krMyqNNZ3RnR2wGJfR4uLUdZlc7LXdCfMymm+3s1Jm+593/9bT+rTjXl6V\nt5aBm1//Ph16vnJGpfnf4Gd0ewdM62Pmi3v6SVlfGdK8WnY1qb7OPoc0qn6ab49D2pRxu8Cl\n+cPfp33Nl87o1KChz+jmGasy2Ww3k/N7+nlZXt6Pl2rZxeHr6HNI9U/jbeN+v8UBQPm33f5r\nDOnfv7//r/V18bUzqkfUmNGrd9+v/Gsv0rRUz4RuyvTk1I/q5ytDeq+/rrefAzqe1m1I3iM9\nw2tn5D3Sc2d8cnLqeLS5OqTTr5//viuSx98PVnjpjHyM9NwZ14b0Vh9FdB3SuvVzqz4jdH+F\nl87IZ+2eO+PaOeX6beCTQ1rWTxe1C+xrFPdWeOmMvrbvjO6dMb32gsL3hjSuD8m/GfgP0+Ex\n0itnNMQRdRBpWUYf2+3i7IHsfp1Ohw3d3tkwJFp3fu2MhjiiLq8jTW6+96r9kHyv3TO07/zS\nGQ1xRF3f2VDeru3XbkOazCOB/zAdOr9yRkMckb+PRADWGRY3gyIBgHWGxc3wHZGOTwOVh6fG\nGOKUvtH5FTMa4ogUiYAi9R8P7QDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD\n4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsM\ni5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwz\nLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDO\nsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6\nw+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDr\nDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCs\nMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACw\nzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDA\nOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA\n6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkA\nrDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQA\nsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEA\nwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUC\nAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4maIivT+7V1Yasaz\nze0lvnsRQJKdf2FGQxxRVKTNKDSkUkbrm0t89yKABDv/xoyGOKKoSNMSGFL1dT0pk5tLfC05\nnHkFm/7GjMpxSWfUYXjLwH773MK4rG4tcVxuMGPK9fyVGZXGcs7ozg5YjMtocXHqukwu9tru\nhHkZzbfbWSmzs3VXuxvH0Wy/1HpaL3Xc76vydjdX8+vfp0PPV86oNP8b/Ixu74BpfYR8cb8+\nKesrQ5pXy64m1dfZybrz/YH2rF5qVH073x6HtCnje7lK84e/T/uaL53RqUFDn9HNM1Zlstlu\nJuf36/OyvLwfL9Wyi8PX0cm6Zbd8daRxXGq8bRwJ3DwkKP+223+NIf379/f/tb4uvnZG9Yga\nM3r17vuVf+1Fmpbqec9NmZ6c+lH9fGVI7/XX9WG3X6x7GNL78dsHQ/Ie6RleOyPvkZ4745OT\nU8ejzdUhnX49WXe9mk8ak2kjksffD1Z46Yx8jPTcGdeG9FYfRbQa0uTruytDWj94btVnhO6v\n8NIZ+azdc2dcO6dcHd2VIX2d9VbGi9X61pCWhyeQbufyNYp7K7x0Rl8bcEb3zphee/ngySE1\n1q1PvDmkcX1I3irXH6bDY6RXzmiII+og0rKMPrbbxdkD2f06jw4bGutWj14/bhx/P/POhiHR\nuvNrZzTEEXV5HWl/4HztnVYPh9RYd3a4dXw/G1K5ufVHuf4u7Tu/dEZDHFHXdzaUt2t78fGQ\nGuu+lTJ5Xx2fkW0MaTLvEPgP06HzK2c0xBH5+0gEYJ1hcTMoEgBYZ1jcDN8R6fg0UHl4aowh\nTukbnV8xoyGOSJEIKFL/8dAOAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxu\nBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4\nGRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPi\nZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyL\nm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMs\nbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6w\nuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD\n4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsM\ni5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwz\nLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDO\nsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6\nw+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDr\nDIubQZEAwDrD4mZQJACwzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCs\nMyxuBkUCAOsMi5tBkQDAOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mZQJACw\nzrC4GRQJAKwzLG4GRQIA6wyLm0GRAMA6w+JmUCQAsM6wuBkUCQCsMyxuBkUCAOsMi5tBkQDA\nOsPiZlAkALDOsLgZFAkArDMsbgZFAgDrDIubQZEAwDrD4mbgiTREXr3T2/HqvfUabu6N39z1\nXegUsFur37uo3u/1drSq80cX7v1IFan/cK7uP7dw70eqSP2Hc3X/uYV7P1JF6j+cq/vPLdz7\nkSpS/+Fc3X9u4d6PVJH6D+fq/nML936kitR/OFf3n1u49yNVpP7Dubr/3MK9H6ki9R/O1f3n\nFu79SBWp/3Cu7j+3cO9Hqkj9h3N1/7mFez9SReo/nKv7zy38x0Yq8hoUSSSAIokEUCSRAIok\nEkCRRAIokkgARRIJoEgiARRJJIAiiQRQJJEA/RZpNiqj2ab1aosOrRbj9he1eSvl7aP9ZW3f\n+73bn+JiNveGdX7eg09bPBvg/avB6cJ3t3wx5HtbPl/4fuZeT3RSJx+3Xe2jw2eWzuqLGrUz\naVSv1N6kzajXu/0pLmZzb1jn533cv1KeDfD+1eB04btbvhjyvS2fL/wgc58n+l5GH9uPUXlv\nt9pujdatPsrbprpte2uz0qxafFambS9sO6V9OvElF7O5N6yL8z7u7rSzAd6/GpwtfG/LF0O+\nt+WLhe9n7rVIs7LafV2Weau1FmXS/no63a/RbsVR2bRep2J556CGwsVs7g3r4rzFvZmeD/Du\n1eB84XtbvhjyvS1fLHw3c79Fmpb19uEtwQVl1uG6/bluhxXLqOUK6y6m942L2dwb1sV5i7K4\nvenzAd69GpwvfHfLh1W+ln/iCtYU6e6W+zzR0uVuYvvRzYeKTZm0Xmf2eHJnTMqaL9LFbO4N\n6+K8aVm97R7JX9/0+QDvXg3OF7675ZrGkB9fwRoLP9hynyfaTaROa+xZ1Hf1bdgdpd0b2jXm\nZdn9LrM3fFekmps3W8+LdH7Goy2fDPnxFayx8IMt93mivy3SetT6eYPFdNTyIVx9IDF0kcru\nxmS7uX1n3l2kR1s+GfLDK9jpwne33OeJ/rJIm1H7A7sdb+2O7cbVE6pDF2nP5uZT2t1FerTl\nkyE/2vKVa8TNLfd5oqPfFWnS+gWrmk2rZxve6mMFvkgXs7k3rFvnPefGo6vBtTNuLnwy5Edb\nvnaNuLVwnye6f1Jl3f6Fmi7X0/V4sm6/VutLe/w3FBlczObesG6d9+zDnvtXgxYinQ35/pav\nXyOIIs3rW+9V64fzXURadXjCbv860rrVWy/+ikgXs7k3rIvzPvfcU248uhqc3X3d2fL5kO9u\n+SIOaGUAAAWLSURBVHzhB5n7PM+O72zoItK6g0f7dzZspq2f//4Lh3bfe2fDrLrubmY3nyRt\n8c6Gs4XvbfliyPe2fLHwg8y9nuj40VOZN2h/PX3rdD8x6pjvD4jUnM2+zb1hnS+82e+52y/K\nlOb/D64GJwvf23JzyA+3fLHwg8y9nuimfnNu+/W6PD3R6YBrl2/c/v7oT4jUmM2+zb1hXV34\nzp47FenB1eBy4RtbLhci3dny9YVvZ+ZPVKQHKJJIAEUSCaBIIgEUSSSAIokEUCSRAIokEkCR\nRAIokkgARRIJoEgiARRJJIAiiQSgiDSaLva/97teTJ/6jIT9e+DHHT6C/0GQ1h8IOXj6MrvN\novpIrenicBmnl/jdjVNE2u2B/acwvz35S0Ofv04y6vhJDDdY7TbZ9sPvhk5PZrcaNTc7XJHG\n+xuz0fjZYVRf15Muv8B6h7cya/dJ+9KT2e1uAt+q3yl/n/7IMQVHpFn991M+dv+3GEb128TR\nO5DdEP7A32T5Xfoxu9HXxlp+EuFzUK4UuwOquv6iLA+7ufo7UPsdstod+u5/YbiU9e72Zr7/\ndr/ian8Hcly6+v3wyf7OfTOuPxPm2pa2q0kpk9XZJW2XZbadVR+52dxQc5OfF/258cYWD0t9\nfsTg7Q8x/Gv0YnbL48f/rGfvn0P6vMghHdpt6j0x/fwE+unX51bMy9enUux2ZfVtc9fsr6/H\npfd/XGr/cafTeq2rW1rsv12crrtb+X37/vX9YUPNTW63nzOqN97Y4tdS0/2n1rT9czVcejG7\n6dlnBe2H9HmRQxJpd0e/rQ+t6tKrMtlsN5Pq7rr+UObl4VMqdqcu6t3/tWuqbxpLL6tv36od\nXi98a0uj6mBkWW2pcf7hU1X3n3B23FBzk5+XeNh4Y4tfSx1uZ99K9mmQ/tKL2Z2r8jWk+iIH\nJdKsvjd425ee1lfmzfH++jCM9+O3jTMaS9e3TLUQ+4VvbunziLp5/rK+xdsf2x031Nzk5wZK\n8wbwkPhzqfH+D5QN5ciuH7P7+pzvw0cDHYf0+dM3S353A7/Erml1LPT1N1FOPj9rvZpPGhO4\nHEZj6eMuO921Z1vaPSyefnycnz+u9/zH6c3mxXcnF3OWraL+22/vgzmy68fsrot0vJBvl/zu\nBn6JXdPqsy8nu+Ohi2FMvr67Moz6EzOfHcZxS9t5dfhcveLQOH/99f36WZHOs1XUt6nzwRzZ\n9WN2zcdIAxepemzydfzUaP5WxovV+tYwlsfHLl8bOvnu+pZ2rGbjs8Pn+ddg5lc2tL0ymYts\nNdXn3o4Hc2TXj9ktGy//DV2kw4uhdenp2d9duzmM+missfTk7AHNjS0dT2icPz7ci9Sfm3/c\n0Nkm30+OJL62eFxqd2w4+RjOkV0/Ztd4HWkzdJGqvwa+PJReVp9/vnu0UT+MfN9+3DjOPrw6\n3lh6UT1TM2vc1F3f0nj/JNC4ef7xr/ZOykdjQ8fvxmVRPUnUFOlri8elquVGwzmy68Xs6nc2\nTOt3NuweQo2GLdK6+eBkf0hcHQjPDodb72fDKF9LNJc+f9HnxpaWX98dzz/+JYL6L4FceR2p\nfgVj2phMY4uN5auhDufIrhez2/H++V67+iBvyCId3nl9KL0Y7/ZIvYfeSpm8r47PcTaGMfk8\ngDouXT+pc/K2xWtb2r86/n5y/uj4Hq36288NNb7bPcx9O5lMY4uN5TdlQEd2vZhdzXI6qja7\n/rygIYr0t1iVAR3ZDQNFegWTn3jbpLwSRfp9Sqc/Tia9RpF+n1H7Py8tfUeRRAIokkgARRIJ\noEgiARRJJIAiiQRQJJEAiiQSQJFEAiiSSABFEgmgSCIBFEkkgCKJBFAkkQCKJBJAkUQCKJJI\nAEUSCaBIIgEUSSSAIokEUCSRAIokEuA/cMqzUdDerDAAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run Random Forest algorithm and print the results.\n",
    "\n",
    "resultRF = eval(instances_df,test_instances_df,failure_incidents,TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
