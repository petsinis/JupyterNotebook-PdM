{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-driven machine learning for PdM focusing on log preprocessing\n",
    "\n",
    "An advanced data-driven predictive maintenance approach is presented in [10].\n",
    "The objective of this research work is to develop an alerting system that provides\n",
    "early noti\f",
    "cations to aviation engineers for upcoming aircraft failures, providing\n",
    "the needed time for the maintenance actions. The aviation is a well-documented\n",
    "field, as all the maintenance and \n",
    "flight data are systematically logged. Hence,\n",
    "**event-based techniques** can leverage this special characteristic and **provide effective predictive solutions**. The **main challenge** is to **cope** with the large set of **og\n",
    "entries** that are essentially irrelevant **to the main failures**.\n",
    "In [10], the emphasis is placed on **log preprocessing**; therefore, we will refer\n",
    "to this methodology as **LPPdM**.\n",
    "\n",
    "10.Korvesis, P., Besseau, S., Vazirgiannis, M.: Predictive maintenance in aviation:\n",
    "Failure prediction from post \n",
    "ight reports. In: IEEE Int. Conf. on Data Engineering\n",
    "(ICDE). pp. 1414{1422 (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'CORElearn' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'dplyr' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'plyr' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'data.table' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'randomForest' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'ggplot2' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'argparser' was built under R version 3.6.2\"Warning message:\n",
      "\"package 'stringr' was built under R version 3.6.2\""
     ]
    }
   ],
   "source": [
    "#Make the necessary imports.\n",
    "\n",
    "suppressMessages(library(CORElearn))\n",
    "suppressMessages(library(dplyr))\n",
    "suppressMessages(library(plyr))\n",
    "suppressMessages(library(data.table))\n",
    "suppressMessages(library(randomForest))\n",
    "suppressMessages(library(ggplot2))\n",
    "suppressMessages(library(grid))\n",
    "suppressMessages(library(argparser))\n",
    "suppressMessages(library(stringr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an argument parser named p and keep there the necessary variables.\n",
    "\n",
    "p <- arg_parser(\"Implementation of the AIRBUS Predictor\")\n",
    "\n",
    "# Add a positional argument\n",
    "p <- add_argument(p, \"id\", help=\"experiment ID\")\n",
    "p <- add_argument(p, \"train\", help=\"training dataset\")\n",
    "p <- add_argument(p, \"test\", help=\"test dataset\")\n",
    "p <- add_argument(p, \"fet\", help=\"different types of the fault events\",default=11)\n",
    "p <- add_argument(p, \"tet\", help=\"type of the target fault events\",default=11)\n",
    "p <- add_argument(p, \"--rre\", help=\"remove rare events\", default=TRUE)\n",
    "p <- add_argument(p, \"--rfe\", help=\"remove frequent events\", default=TRUE)\n",
    "p <- add_argument(p, \"--kofe\", help=\"keep only first event\", default=TRUE)\n",
    "p <- add_argument(p, \"--milt\", help=\"MIL as written in the text of the paper\", default=TRUE)\n",
    "p <- add_argument(p, \"--mili\", help=\"MIL as shonw in the Figure of the paper\", default=FALSE)\n",
    "p <- add_argument(p, \"--milthres\", help=\"MIL threshold to the sigmoid function for over-sampling\", default=0.4)\n",
    "p <- add_argument(p, \"--steepness\", help=\"steepness of the sigmoid function\", default=0.7)\n",
    "p <- add_argument(p, \"--midpoint\", help=\"midpoint of the sigmoid function (in days)\", default=11)\n",
    "p <- add_argument(p, \"--fs\", help=\"apply feature selection\", default=FALSE)\n",
    "p <- add_argument(p, \"--top\", help=\"# of features to keep in feature selection\", default=3)#we have max 10 feautures(before was 200)\n",
    "p <- add_argument(p, \"--rer\", help=\"rare events ratio of the target event frequency\", default=0.5)\n",
    "p <- add_argument(p, \"--fer\", help=\"frequent events ratio of the frequency of the most frequent event\", default=0.8)\n",
    "p <- add_argument(p, \"--milw\", help=\"MIL window size (in days)\", default=6)\n",
    "p <- add_argument(p, \"--pthres\", help=\"prediction threshold to the Risk value for a true positive episode\", default=0.5)\n",
    "p <- add_argument(p, \"--seed\", help=\"seed for RF\", default=400)\n",
    "p <- add_argument(p, \"--csv\", help=\"output for csv\", default=TRUE)\n",
    "\n",
    "\n",
    "p <- add_argument(p, \"--spme\", help=\"export datasets for sequential pattern minning\", default=FALSE)\n",
    "\n",
    "setwd(\"C:/Program Files (x86)\")\n",
    "p <- add_argument(p, \"--java\", help=\"the java path\", default=\"./Java/jdk1.8.0_192/bin/java.exe\")\n",
    "\n",
    "setwd(\"C:/Users/PETROS PETSINIS\")\n",
    "p <- add_argument(p, \"--python\", help=\"the python path\", default=\"./Anaconda/python.exe\")\n",
    "\n",
    "setwd(\"C:\")\n",
    "p <- add_argument(p, \"--cep\", help=\"complex event processing path\", default=\"C:/Users/Public/ptyxiakh/my_spmrules.py\")\n",
    "p <- add_argument(p, \"--spmf\", help=\"the spmf path\", default=\"C:/Users/Public/ptyxiakh/spmf.jar\")\n",
    "\n",
    "\n",
    "p <- add_argument(p, \"--conf\", help=\"minimum support (minsup)\", default=\"70%\")\n",
    "p <- add_argument(p, \"--minti\", help=\"minimum time interval allowed between two succesive itemsets of a sequential pattern\", default=4)\n",
    "p <- add_argument(p, \"--maxti\", help=\"maximum time interval allowed between two succesive itemsets of a sequential pattern\", default=5)\n",
    "p <- add_argument(p, \"--minwi\", help=\"minimum time interval allowed between the first itemset and the last itemset of a sequential pattern\", default=4)\n",
    "p <- add_argument(p, \"--maxwi\", help=\"maximum time interval allowed between the first itemset and the last itemset of a sequential pattern\", default=5)\n",
    "p <- add_argument(p, \"--minwint\", help=\"min # of days before failure to expect a warning for true positive decision\", default=2)\n",
    "p <- add_argument(p, \"--maxwint\", help=\"max # of days before failure to expect a warning for true positive decision\", default=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The data frame argv is:\"\n",
      "[[1]]\n",
      "[1] FALSE\n",
      "\n",
      "$help\n",
      "[1] FALSE\n",
      "\n",
      "$opts\n",
      "[1] NA\n",
      "\n",
      "$rre\n",
      "[1] TRUE\n",
      "\n",
      "$rfe\n",
      "[1] TRUE\n",
      "\n",
      "$kofe\n",
      "[1] TRUE\n",
      "\n",
      "$milt\n",
      "[1] TRUE\n",
      "\n",
      "$mili\n",
      "[1] FALSE\n",
      "\n",
      "$milthres\n",
      "[1] 0.4\n",
      "\n",
      "$steepness\n",
      "[1] 0.7\n",
      "\n",
      "$midpoint\n",
      "[1] 11\n",
      "\n",
      "$fs\n",
      "[1] FALSE\n",
      "\n",
      "$top\n",
      "[1] 3\n",
      "\n",
      "$rer\n",
      "[1] 0.5\n",
      "\n",
      "$fer\n",
      "[1] 0.8\n",
      "\n",
      "$milw\n",
      "[1] 6\n",
      "\n",
      "$pthres\n",
      "[1] 0.5\n",
      "\n",
      "$seed\n",
      "[1] 400\n",
      "\n",
      "$csv\n",
      "[1] TRUE\n",
      "\n",
      "$spme\n",
      "[1] FALSE\n",
      "\n",
      "$java\n",
      "[1] \"./Java/jdk1.8.0_192/bin/java.exe\"\n",
      "\n",
      "$python\n",
      "[1] \"./Anaconda/python.exe\"\n",
      "\n",
      "$cep\n",
      "[1] \"C:/Users/Public/ptyxiakh/spm_rules.py\"\n",
      "\n",
      "$spmf\n",
      "[1] \"C:/Users/Public/ptyxiakh/spmf.jar\"\n",
      "\n",
      "$conf\n",
      "[1] \"20%\"\n",
      "\n",
      "$minti\n",
      "[1] 2\n",
      "\n",
      "$maxti\n",
      "[1] 4\n",
      "\n",
      "$minwi\n",
      "[1] 2\n",
      "\n",
      "$maxwi\n",
      "[1] 4\n",
      "\n",
      "$minwint\n",
      "[1] 2\n",
      "\n",
      "$maxwint\n",
      "[1] 5\n",
      "\n",
      "$id\n",
      "[1] \"1\"\n",
      "\n",
      "$train\n",
      "[1] \"C:/Users/Public/ptyxiakh/training_my_dataset.csv\"\n",
      "\n",
      "$test\n",
      "[1] \"C:/Users/Public/ptyxiakh/testing_my_dataset.csv\"\n",
      "\n",
      "$fet\n",
      "[1] 11\n",
      "\n",
      "$tet\n",
      "[1] 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the necessary variables.\n",
    "\n",
    "argv = data.frame() #make a data frame named argv\n",
    "#if( length(commandArgs(trailingOnly = TRUE)) != 0){\n",
    "if(FALSE){\n",
    "  argv <- parse_args(p)\n",
    "} else {\n",
    "  argv <- parse_args(p,c(1,\"C:/Users/Public/ptyxiakh/training_my_dataset2.csv\",\"C:/Users/Public/ptyxiakh/testing_my_dataset2.csv\",11,11))\n",
    "}\n",
    "\n",
    "\n",
    "#init the variables\n",
    "id = argv$id\n",
    "\n",
    "train_path=argv$train\n",
    "test_path=argv$test\n",
    "\n",
    "b_length = argv$fet\n",
    "target_event = argv$tet\n",
    "\n",
    "target_event_frequency_proportion_rare = argv$rer\n",
    "max_event_frequency_proportion_frequent = argv$fer\n",
    "\n",
    "top_features = argv$top\n",
    "\n",
    "milw = argv$milw\n",
    "F_thres = argv$milthres\n",
    "\n",
    "s = argv$steepness\n",
    "midpoint = argv$midpoint\n",
    "acceptance_threshold = argv$pthres\n",
    "\n",
    "export_spm = argv$spme\n",
    "\n",
    "max_warning_interval = argv$maxwint\n",
    "min_warning_interval = argv$minwint\n",
    "\n",
    "csv = argv$csv\n",
    "\n",
    "seed = argv$seed\n",
    "\n",
    "print(\"The data frame argv is:\")\n",
    "print(argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading function\n",
    "**function: read_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading the csv file and save it to a two column table.\n",
    "\n",
    "read_dataset <- function(path){\n",
    "  dataset = read.table(path, header = TRUE, sep = \",\", dec = \".\", comment.char = \"#\")\n",
    "  dataset[, 2]  <- as.numeric(dataset[, 2])\n",
    "  return(dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test set\n",
    "\n",
    "The recorded log types read from csv files.\n",
    "One csv file(at **train_path**) has the **training_set** and the other(at **test_path**) the **testing_set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train and test set.\n",
    "\n",
    "training_set = read_dataset(train_path)\n",
    "test_set =  read_dataset(test_path)\n",
    "\n",
    "print(\"The test_set and training_set looks like:\")\n",
    "print(head(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtions for preprocessing\n",
    "\n",
    "In order to increase the effectiveness of the approach standard **preprocessing** techniques are applied: \n",
    "\n",
    "- **Rare events (more rare than the target event)**, are considered as extremely rare, hence they are removed to reduce the dimensionality of the data. \n",
    "\n",
    "- **Most frequent events** usually do not contain significant information since they correspond to issues of minor importance. A tf-idf (term-frequency - inverted document frequency) or a simple threshold-based approach can be used to remove most frequent events. \n",
    "\n",
    "- **Multiple occurrences** of the same event in the same segment can either be noise or may not provide useful information. Hence, multiple occurrences are shrank into a single one. \n",
    "\n",
    "- Events of minor importance occur and appear in every segment until their underlying cause is treated by the technical experts. Hence, the **first occurrence of events** that occur in consecutive segments is maintained. \n",
    "\n",
    "- To deal with the imbalance of the labels (given that the target event is rare) and as several events appear shortly before the occurrence of the target event, but only a small subset of them is related to the target event, the authors use **Multiple Instance Learning (MIL) bagging the events**. A single bag contains fault events of a single day. Using MIL, the data closer to the target event (a threshold is specified), are over-sampled.\n",
    "\n",
    "- A statistical **feature selection technique**, based on the distance of the fault events with the target event is applied, to filter out fault events, which are far from the target event.\n",
    "\n",
    "\n",
    "\n",
    "**1) function: preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for preprocessing the data set.\n",
    "\n",
    "preprocess <- function(ds,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS,KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT,MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION,top_features,s,midpoint,b_length,target_event,target_event_frequency_proportion_rare,max_event_frequency_proportion_frequent,w,F_thres){\n",
    "  \n",
    "  #remove events that appear < n times. We consider n = (target event frequency)/2\n",
    "  if(REMOVE_RARE_EVENTS){\n",
    "    ds<-remove_rare_events(ds,target_event_frequency_proportion_rare) #function: remove_rare_events\n",
    "  }\n",
    "  \n",
    "  #remove events that appear > n times. We consider n = (target event frequency)/2\n",
    "  if(REMOVE_FREQUENT_EVENTS){\n",
    "    ds<-remove_frequent_events(ds,max_event_frequency_proportion_frequent) #function: remove_frequent_events\n",
    "  }\n",
    "  \n",
    "  #create for the dataset(ds) a dataframe keeping for each day the frequency of the fault events\n",
    "  episodes_list = create_episodes_list(ds,target_event,b_length,s,midpoint) #function: create_episodes_list\n",
    "  print(\"The starting episodes_list is:\")\n",
    "  print(episodes_list)\n",
    "  print(\"------------------------------------------------------------------------------------------------------\")\n",
    "  \n",
    "  #binarize the vector\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    ep = episodes_list[[ep_index]]\n",
    "    ep[2:(ncol(ep)-1)][ep[2:(ncol(ep)-1)] > 0] = 1\n",
    "    episodes_list[[ep_index]] = ep\n",
    "  }\n",
    "  print(\"After binirizing:\")\n",
    "  print(episodes_list)\n",
    "  print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  \n",
    "  \n",
    "  #keep only the first occurness of event in consecutive segments\n",
    "  if(KEEP_ONLY_FIRST_OCCURENESS){\n",
    "    episodes_list <- keep_only_first_occureness(episodes_list) #function: keep_only_first_occureness\n",
    "    print(\"After first occurancing preprocess:\")\n",
    "    print(episodes_list)\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  }\n",
    "    \n",
    "  \n",
    "  \n",
    "  #multi-instance learning to increase the pattern frequency\n",
    "  if(MULTI_INSTANCE_LEARNING_TEXT){\n",
    "    episodes_list <- mil_text(w,F_thres,episodes_list,b_length) #function: mil_text\n",
    "    print(\"After MIL text (the returning list):\")\n",
    "    print(episodes_list)\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  } else if(MULTI_INSTANCE_LEARNING_IMAGE){\n",
    "    episodes_list <- mil_image(w,F_thres,episodes_list,b_length) #function: mil_image\n",
    "    print(\"After MIL image (the returning list):\")\n",
    "    print(episodes_list)\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  }\n",
    "    \n",
    "      \n",
    "  \n",
    "  return(episodes_list)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Functions for removing the less useful data\n",
    "\n",
    "**2a) function: remove_rare_events**\n",
    "\n",
    "**2b) function: remove_frequent_events**\n",
    "\n",
    "**2c) function: keep_only_first_occureness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for removing events.\n",
    "\n",
    "#Function for removing the rare events, considering target event's frequency.\n",
    "remove_rare_events <- function(ds,target_event_frequency_proportion_rare){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: REMOVE RARE EVENTS~~~~~~~\")\n",
    "  }\n",
    "  a = table(ds$Event_id) #table that shows the total appearances per day\n",
    "\n",
    "  target_event_frequency = a[names(a)==target_event] #total appearances of target event\n",
    "\n",
    "  #find the rear events :the events that their frequency is smaller than a threashold\n",
    "  #threashold           :percentage of total appearances of target event\n",
    "  #percentage           :given by the user usually near 0.5 (target_event_frequency_proportion_rare) \n",
    "  rare_events = as.integer(names(a[a < target_event_frequency*target_event_frequency_proportion_rare])) \n",
    "  \n",
    "  print(\"The removing rare events(columns) are:\")\n",
    "  print(rare_events)  \n",
    "  print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "    \n",
    "  return(ds[!(ds$Event_id %in% rare_events),]) #return the dataset without the rear events\n",
    "}\n",
    "\n",
    "#Function for removing the frequent events, considering the maximum frequency that observed.\n",
    "remove_frequent_events <- function(ds,max_event_frequency_proportion_frequent){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: REMOVE FREQUENT EVENTS~~~~~~~\")\n",
    "  }\n",
    "  a = table(ds$Event_id) #table that shows the total appearances per day\n",
    "\n",
    "  max_freq = sort(a,decreasing = TRUE)[[1]] #maximum total appearances of a target event\n",
    "\n",
    "  #find the rear events :the events that their frequency is bigger than a threashold\n",
    "  #threashold           :percentage of the maximum total appearances of a target event\n",
    "  #percentage           :given by the user usually near 0.5 (max_event_frequency_proportion_frequent)  \n",
    "  frequent_events = as.integer(names(a[a > max_freq*max_event_frequency_proportion_frequent]))\n",
    "\n",
    "  print(\"The removing frequent_events events(columns) are:\")\n",
    "  print(frequent_events)  \n",
    "  print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "  return(ds[!(ds$Event_id %in% frequent_events),])\n",
    "}\n",
    "\n",
    "#Function for keeping oly the first occureness of consecutively events.\n",
    "keep_only_first_occureness <- function(episodes_list){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: KEEP ONLY FIRST OCCURENESS~~~~~~~\")\n",
    "  }\n",
    "  #for every episode in the episodes_list\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    #keep the episode  \n",
    "    ep = episodes_list[[ep_index]]\n",
    "      \n",
    "    #For every segment of each episode starting from the end up to the second segment. \n",
    "    #We need to keep only the 1st occurness of consequtive events, hence starting from the end is the easy way.\n",
    "    for(i in (nrow(ep):2)){\n",
    "      #as we deal with binary vectors, to find the indeces that both vectors have \"1\" we sum them and check for \"2\"s in the result\n",
    "      matches = which((ep[i,]+ep[i-1,]) %in% c(2))\n",
    "        \n",
    "      #replace the 1s with 0s in the matching positions of the segment that is closer to the end of the episode\n",
    "      ep[i,][c(matches)] = 0\n",
    "    }\n",
    "    #save changes to the episodes_list\n",
    "    episodes_list[[ep_index]] = ep\n",
    "  }\n",
    "  return(episodes_list) #return the new episodes_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Functions to compute episodes list\n",
    "\n",
    "The post flight logs are partitioned in ranges\n",
    "defined by the occurrences of the fault(**episodes**) that PdM targets. These ranges are further\n",
    "partitioned into time-segments, which may correspond to a day or to a single\n",
    "usage of the equipment(a **day** for our example). The idea is that the segments that are closer to the end\n",
    "of the range may contain fault events that are potentially indicative of the main\n",
    "event. The **goal** is to **learn a function** that quantifies the **risk** of the targeted\n",
    "failure occurring in the near future, given the events that precede it. Hence, a\n",
    "**sigmoid function** is proposed, which **maps higher values to the segments that are\n",
    "closer to the target event**. The **steepness** and **shift** of the sigmoid function are\n",
    "configured to better map the expectation of the time before the target event at\n",
    "which correlated events will start occurring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode explanation\n",
    "\n",
    "For the **test_set** the episodes are presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"For testing set the freq list is:\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "1  2014-01-21   1   0   1   2   1   1   2   0   0    0    0\n",
      "2  2014-01-22   1   1   1   1   2   0   1   0   0    0    0\n",
      "3  2014-01-23   1   1   1   1   1   2   0   1   0    0    0\n",
      "4  2014-01-24   0   0   1   1   2   0   1   0   0    1    0\n",
      "5  2014-01-25   1   1   2   1   1   2   1   0   1    0    0\n",
      "6  2014-01-26   1   1   0   1   1   1   2   0   0    0    1\n",
      "7  2014-01-27   0   1   2   1   1   1   1   0   1    0    0\n",
      "8  2014-01-28   1   1   1   1   1   0   1   0   0    1    0\n",
      "9  2014-01-29   0   0   1   2   0   0   2   0   0    0    0\n",
      "10 2014-01-30   1   1   2   1   2   2   0   0   0    0    1\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The episode 1 is:\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "1 2014-01-21   1   0   1   2   1   1   2   0   0    0    0\n",
      "2 2014-01-22   1   1   1   1   2   0   1   0   0    0    0\n",
      "3 2014-01-23   1   1   1   1   1   2   0   1   0    0    0\n",
      "4 2014-01-24   0   0   1   1   2   0   1   0   0    1    0\n",
      "5 2014-01-25   1   1   2   1   1   2   1   0   1    0    0\n",
      "6 2014-01-26   1   1   0   1   1   1   2   0   0    0    0\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The episode 2 is:\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11\n",
      "7  2014-01-27   0   1   2   1   1   1   1   0   1    0    0\n",
      "8  2014-01-28   1   1   1   1   1   0   1   0   0    1    0\n",
      "9  2014-01-29   0   0   1   2   0   0   2   0   0    0    0\n",
      "10 2014-01-30   1   1   2   1   2   2   0   0   0    0    0\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n"
     ]
    }
   ],
   "source": [
    "#Function for creating frequency vectors for each day.\n",
    "\n",
    "compute_frequency_list <- function(ds2years,b_length){\n",
    "\n",
    "  #data.frame for frequencies\n",
    "  episode_df <- data.frame(Timestamps=as.Date(character()),Event_id=integer())\n",
    "  \n",
    "  #Change ds2years(table) to episode_df(data frame)    \n",
    "    \n",
    "  #iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(ds2years)) {\n",
    "    #get the current row of ds2years(table of data set)\n",
    "    meas <- ds2years[i,]\n",
    "    #add it to data frame  \n",
    "    episode_df <- rbind(episode_df,data.frame(Timestamps=meas$Timestamps, Event_id=meas$Event_id))\n",
    "\n",
    "  }\n",
    "  #group by day\n",
    "  aggr_episode_df = aggregate(episode_df[ ,2], FUN=function(x){return(x)}, by=list(as.Date(episode_df$Timestamps, \"%Y-%m-%d\")))\n",
    "  \n",
    "  #binarize the frequncy vector(function: compute_frequency_vectors)\n",
    "  frequency_day_vectors = compute_frequency_vector(aggr_episode_df,b_length)\n",
    "\n",
    "  return(frequency_day_vectors)\n",
    "}\n",
    "\n",
    "#Convert event vectors to binary vectors\n",
    "\n",
    "compute_frequency_vector <- function(aggr_episode_df,b_length){\n",
    "    \n",
    "  #data frame for binary frequency vectors  \n",
    "  freq_aggr_episode_df <- data.frame(matrix(ncol = b_length+1, nrow = 0))\n",
    "  \n",
    "  #x keeps the names of the columns. |Timestamps||e_1||e_2|...|e_b_length|  \n",
    "  x <- c(c(\"Timestamps\"), c(paste(\"e_\",c(1:b_length),sep = \"\")))\n",
    "\n",
    "  #iterate over every line(day) of the aggr_episode_df\n",
    "  for(i in 1:nrow(aggr_episode_df)) {\n",
    "      \n",
    "      #init a vector with b_length zeros\n",
    "      freq_vector = as.vector(integer(b_length))\n",
    "    \n",
    "      #get the current row of aggr_episode_df(frequency vector-data frame of data set)\n",
    "      seg <- aggr_episode_df[i,]\n",
    "    \n",
    "      #for every value(fault event) in the current line(that happened in the current day)\n",
    "      for(value in seg$x[[1]]){\n",
    "          #replace the 0 in freq_vector with 1 at \"value=fault event\" position \n",
    "          freq_vector[[value]] = length(which(seg$x[[1]] == value))\n",
    "      }\n",
    "    \n",
    "      #add a new line to the bin_aggr_epissode_df\n",
    "      #we use a matrix holding the elements of the new_data.frame as matrix is able to store variable of different data types\n",
    "      \n",
    "      date = as.Date(seg$Group.1[[1]])\n",
    "      new_df = data.frame(matrix(c(date, freq_vector),nrow=1,ncol=b_length+1))\n",
    "      freq_aggr_episode_df <- rbind(freq_aggr_episode_df,new_df)\n",
    "  }\n",
    "  #set column's name as x defines\n",
    "  colnames(freq_aggr_episode_df) <- x\n",
    "  \n",
    "  #set column \"Timestamps\" x to a Date: \"Y-m-d\" column  \n",
    "  freq_aggr_episode_df$Timestamps <- as.Date(freq_aggr_episode_df$Timestamps , origin=\"1970-01-01\")\n",
    "    \n",
    "  return(freq_aggr_episode_df)\n",
    "}\n",
    "\n",
    "\n",
    "freq_df=compute_frequency_list(test_set,b_length)\n",
    "print(\"For testing set the freq list is:\")\n",
    "print(freq_df)\n",
    "print(\"------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "epcnt=0\n",
    "before=0\n",
    "\n",
    "#split the dataframe to episodes \n",
    "#episode :the next day of a target event(or the start of the data frame) until the next target event(or the end of the data frame)\n",
    "#        :target event frequency at episode is equal to zero\n",
    "\n",
    "#iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(freq_df)) {\n",
    "    #get the current row of the ds\n",
    "    meas <- freq_df[i,]  \n",
    "    #if it is the target event enable the appropriate flag\n",
    "    if(meas[1,target_event+1] == 1){\n",
    "      epcnt=epcnt+1\n",
    "      freq_df[i,target_event+1] = 0  \n",
    "      str=paste(\"The episode \",epcnt,\" is:\",sep=\"\")  \n",
    "      print(str)    \n",
    "      print(freq_df[before:i,])\n",
    "      freq_df[i,target_event+1] = 1\n",
    "        \n",
    "      before=i+1\n",
    "      print(\"------------------------------------------------------------------------------------------------------\") \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a) function: create_episodes_list**\n",
    "\n",
    "**3b) function: compute_frequency_vectors**\n",
    "\n",
    "**3c) function: compute_F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for computing episodes list.\n",
    "\n",
    "#Function for creating episodes list of each day's frequency vectors.\n",
    "create_episodes_list <- function(ds,target_event,b_length,s,midpoint){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~CREATING FREQUENCY VECTORS AND BINARIZE THEM~~~~~~~\")\n",
    "  }\n",
    "  #devide in episodes\n",
    "  target_event_spotted = FALSE\n",
    "    \n",
    "  #a list with data.frames for the episodes (each episode one data.frame)\n",
    "  episodes_list = list()\n",
    "    \n",
    "  #data.frame for episodes\n",
    "  episode_df <- data.frame(Timestamps=as.POSIXct(character()),Event_id=integer())\n",
    "    \n",
    "  #iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(ds)) {\n",
    "    #get the current row of the ds\n",
    "    meas <- ds[i,]\n",
    "    #if it is the target event enable the appropriate flag\n",
    "    if((meas$Event_id == target_event) || i==1 ){\n",
    "      target_event_spotted = TRUE\n",
    "    }\n",
    "      \n",
    "    #fill the episode data.frame with the events that are between two target events\n",
    "    if(meas$Event_id != target_event && target_event_spotted){\n",
    "      episode_df <- rbind(episode_df,data.frame(Timestamps=meas$Timestamps, Event_id=meas$Event_id))\n",
    "    } else if(meas$Event_id == target_event && target_event_spotted && is.data.frame(episode_df) && nrow(episode_df) != 0){\n",
    "      #a second occurness of the target event is spotted, close the episode\n",
    "        \n",
    "      #aggregate by day all the events to form the segments inside the episodes\n",
    "      aggr_episode_df = aggregate(episode_df[ ,2], FUN=function(x){return(x)}, by=list(Timeframe=cut(as.POSIXct(episode_df$Timestamps, format=\"%Y-%m-%d\"),\"day\"))) #%Y-%m-%dT%H:%M:%OSZ\n",
    "\n",
    "      #binarize the frequncy vector\n",
    "      bin_aggr_episode_df = compute_frequency_vectors(aggr_episode_df,b_length,s,midpoint) #function: compute_frequency_vectors\n",
    "\n",
    "      #add the episode to the episodes_list\n",
    "      episodes_list[[length(episodes_list)+1]] = bin_aggr_episode_df\n",
    "        \n",
    "      #reset episode_df to en empty data.frame\n",
    "      episode_df <- data.frame(Timestamps=as.POSIXct(character()),Event_id=integer())\n",
    "    }\n",
    "  }\n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "#Convert event vectors to binary vectors\n",
    "compute_frequency_vectors <- function(aggr_episode_df,b_length,s,midpoint){\n",
    "  \n",
    "  #data frame for binary frequency vectors   \n",
    "  freq_aggr_episode_df <- data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    \n",
    "  #x keeps the names of the columns. |Timestamps||e_1||e_2|...|Risk_F|   \n",
    "  x <- c(c(\"Timestamps\"), c(paste(\"e_\",c(1:b_length),sep = \"\")), c(\"Risk_F\"))\n",
    "\n",
    "  \n",
    "  for(i in 1:nrow(aggr_episode_df)) {\n",
    "      \n",
    "    #init a vector with b_length zeros\n",
    "    freq_vector = as.vector(integer(b_length))\n",
    "      \n",
    "    #get the current row of aggr_episode_df(frequency vector-data frame of data set)  \n",
    "    seg <- aggr_episode_df[i,]\n",
    "      \n",
    "    #for every value(fault event) in the current line(that happened in the current day)\n",
    "    for(value in seg$x[[1]]){\n",
    "      #replace the 0 in freq_vector with 1 at \"value=fault event\" position   \n",
    "      freq_vector[[value]] = length(which(seg$x[[1]] == value))\n",
    "    }\n",
    "    #add a new line to the bin_aggr_epissode_df\n",
    "    #we use a matrix holding the elements of the new_data.frame as matrix is able to store variable of different data types\n",
    "    \n",
    "    F = compute_F(s,midpoint,i-1,nrow(aggr_episode_df)) #compute risk F (function: compute_F)\n",
    "    date = seg$Timeframe[[1]]\n",
    "    new_df = data.frame(matrix(c(date, freq_vector,F),nrow=1,ncol=b_length+2))\n",
    "    freq_aggr_episode_df <- rbind(freq_aggr_episode_df,new_df)\n",
    "  }\n",
    "  #set column's name as x defines  \n",
    "  colnames(freq_aggr_episode_df) <- x\n",
    "    \n",
    "  return(freq_aggr_episode_df)\n",
    "}\n",
    "\n",
    "#The Risk function(sigmoid)\n",
    "compute_F <- function(s,midpoint,t,ep_length){\n",
    "  return(1/(1+exp(s*(ep_length-midpoint-t)))) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Multi Instance Learning functions\n",
    "\n",
    "**4a) function: mil_text**\n",
    "\n",
    "**4b) function: mil_image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mil_text <- function(milw,F_thres,episodes_list,b_length){\n",
    "  print(\"STARTIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIING!!!!\")\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "  cat(\"     window_df\")\n",
    "  print(window_df)\n",
    "  #for every episode in the episodes_list\n",
    "  cat(\"    length of the episodes list\")\n",
    "  print(length(episodes_list))\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "  #for(ep_index in (2:2)){\n",
    "    ep = episodes_list[[ep_index]]\n",
    "    cat(\"------------------------------------------------ep is:\")\n",
    "    print(\"\\n\")\n",
    "    print(ep)\n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    cat(\"------------------------------------------------the arxikopoihsh of the new ep is\")\n",
    "    print(\"\\n\")\n",
    "    print(new_ep)\n",
    "    i = 1\n",
    "    cat(\"------------------------------------------------nrow(ep)\")\n",
    "    cat(\"\")\n",
    "    print(nrow(ep))\n",
    "    while(i <= nrow(ep)){\n",
    "      new_ep = rbind(new_ep,ep[i,])\n",
    "      cat(\"------------------------------------------------new_ep\")\n",
    "      print(\"\\n\")\n",
    "      print(new_ep)\n",
    "      if(ep[i,][b_length+2] >= F_thres && nrow(window_df) < milw){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "        cat(\"------------------------------------------------window_df changed to\")\n",
    "        print(\"\\n\")\n",
    "        print(window_df)\n",
    "      }\n",
    "      if(nrow(window_df) == milw || i == nrow(ep)){\n",
    "        mean = colMeans(window_df)#mean of each column\n",
    "        cat(\"                      mean \")\n",
    "        print(\"\\n\")\n",
    "        print(mean)\n",
    "        mean[mean > 0] = 1#binirize the mean of each column\n",
    "        cat(\"                      new mean \")\n",
    "        print(\"\\n\")\n",
    "        print(mean)\n",
    "        mf = data.frame(as.list(mean))#make list to frame\n",
    "        cat(\"                      mf \")\n",
    "        print(\"\\n\")\n",
    "        print(mf)\n",
    "        mf[1] = ep[i,][1]#timestamp changed from 1 to i or row that are at the currend loop\n",
    "        mf[b_length+2] = ep[i,][b_length+2]#RISK_F changed from 1 to the risk of the current i \n",
    "        cat(\"                      new mf \")\n",
    "        print(\"\\n\")\n",
    "        print(mf)\n",
    "        #colnames(mf) = colnames(new_ep)\n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        cat(\"------------------------------------------------2 new_ep\")\n",
    "        print(\"\\n\")\n",
    "        print(new_ep)\n",
    "        if(nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-2)#4-(4=+2)\n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "        cat(\"------------------------------------------------2 window_df changed to\")\n",
    "        print(\"\\n\")\n",
    "        print(window_df)\n",
    "      }\n",
    "      i = i + 1#2+1=3,3+1=4\n",
    "    }\n",
    "    episodes_list[[ep_index]] = new_ep\n",
    "  }\n",
    "  cat(\"------------------------------------------------returning episodes list\")\n",
    "  print(\"\\n\")\n",
    "  print(episodes_list)\n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "mil_image <- function(milw,F_thres,episodes_list,b_length){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  \n",
    "  #for every episode in the episodes_list\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    ep = episodes_list[[ep_index]]\n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    #a data.frame with the vectors that need to be averaged\n",
    "    window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    i = 1\n",
    "    while(i <= nrow(ep)){\n",
    "      #new_ep = rbind(new_ep,ep[i,])\n",
    "      if(nrow(window_df) < milw){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      if(nrow(window_df) == milw || i == nrow(ep)){\n",
    "        mean = colMeans(window_df)\n",
    "        mean[mean > 0] = 1\n",
    "        mf = data.frame(as.list(mean))\n",
    "        mf[1] = ep[i,][1]\n",
    "        mf[b_length+2] = ep[i,][b_length+2]\n",
    "        #colnames(mf) = colnames(new_ep)\n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        if(window_df[1,][b_length+2] >= F_thres && nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-1)\n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "      }\n",
    "      i = i + 1\n",
    "    }\n",
    "    episodes_list[[ep_index]] = new_ep\n",
    "  }\n",
    "  return(episodes_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The removing rare events(columns) are:\"\n",
      "[1] 9\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The removing frequent_events events(columns) are:\"\n",
      "[1] 3 4 5 7\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The starting episodes_list is:\"\n",
      "[[1]]\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   1   0   0   0   2   0   2   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   1   0   1   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   2   0   0   0    0    0 1.0000000\n",
      "5           5   1   1   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   1   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           7   1   1   0   0   0   1   0   2   0    1    0 1.0000000\n",
      "8           8   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "9           9   1   1   0   0   0   1   0   0   0    1    0 1.0000000\n",
      "10         10   1   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   1   0    1    0      1\n",
      "3          3   1   1   0   0   0   1   0   1   0    0    0      1\n",
      "4          4   0   0   0   0   0   1   0   0   0    0    0      1\n",
      "5          5   1   1   0   0   0   1   0   0   0    0    0      1\n",
      "\n",
      "[[3]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   0   0    1    0      1\n",
      "3          3   1   1   0   0   0   0   0   1   0    0    0      1\n",
      "4          4   1   1   0   0   0   2   0   0   0    0    0      1\n",
      "\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After binirizing:\"\n",
      "[[1]]\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   1   0   0   0   1   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   1   0   1   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "5           5   1   1   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   1   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           7   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "8           8   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "9           9   1   1   0   0   0   1   0   0   0    1    0 1.0000000\n",
      "10         10   1   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   1   0    1    0      1\n",
      "3          3   1   1   0   0   0   1   0   1   0    0    0      1\n",
      "4          4   0   0   0   0   0   1   0   0   0    0    0      1\n",
      "5          5   1   1   0   0   0   1   0   0   0    0    0      1\n",
      "\n",
      "[[3]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   0   0    1    0      1\n",
      "3          3   1   1   0   0   0   0   0   1   0    0    0      1\n",
      "4          4   1   1   0   0   0   1   0   0   0    0    0      1\n",
      "\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After first occurancing preprocess:\"\n",
      "[[1]]\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   1   0    1    0      0\n",
      "3          3   0   1   0   0   0   0   0   0   0    0    0      0\n",
      "4          4   0   0   0   0   0   0   0   0   0    0    0      0\n",
      "5          5   1   1   0   0   0   0   0   0   0    0    0      0\n",
      "\n",
      "[[3]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   0   0   0   0   0   0   0    0    0      1\n",
      "2          2   1   0   0   0   0   1   0   0   0    1    0      0\n",
      "3          3   0   1   0   0   0   0   0   1   0    0    0      0\n",
      "4          4   0   0   0   0   0   1   0   0   0    0    0      0\n",
      "\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"STARTIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIING!!!!\"\n",
      "     window_df [1] X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13\n",
      "<0 rows> (or 0-length row.names)\n",
      "    length of the episodes list[1] 3\n",
      "------------------------------------------------ep is:[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "------------------------------------------------the arxikopoihsh of the new ep is[1] \"\\n\"\n",
      " [1] X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13\n",
      "<0 rows> (or 0-length row.names)\n",
      "------------------------------------------------nrow(ep)[1] 10\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "                      mean [1] \"\\n\"\n",
      "Timestamps        e_1        e_2        e_3        e_4        e_5        e_6 \n",
      " 3.5000000  0.3333333  0.3333333  0.0000000  0.0000000  0.0000000  0.3333333 \n",
      "       e_7        e_8        e_9       e_10       e_11     Risk_F \n",
      " 0.0000000  0.3333333  0.0000000  0.1666667  0.0000000  0.9995869 \n",
      "                      new mean [1] \"\\n\"\n",
      "Timestamps        e_1        e_2        e_3        e_4        e_5        e_6 \n",
      "         1          1          1          0          0          0          1 \n",
      "       e_7        e_8        e_9       e_10       e_11     Risk_F \n",
      "         0          1          0          1          0          1 \n",
      "                      mf [1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   1   1   0   0   0   1   0   1   0    1    0      1\n",
      "                      new mf [1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          6   1   1   0   0   0   1   0   1   0    1    0      1\n",
      "------------------------------------------------2 new_ep[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1          1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2          2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7          6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "------------------------------------------------2 window_df changed to[1] \"\\n\"\n",
      " [1] X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13\n",
      "<0 rows> (or 0-length row.names)\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0      1\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0      1\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0      1\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0      1\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0      1\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0      1\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "------------------------------------------------window_df changed to[1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "3          3   0   0   0   0   0   0   0   0   0    0    0      1\n",
      "4          4   1   1   0   0   0   0   0   0   0    0    0      1\n",
      "5          5   0   0   0   0   0   0   0   1   0    1    0      1\n",
      "6          6   0   0   0   0   0   1   0   0   0    0    0      1\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "                      mean [1] \"\\n\"\n",
      "Timestamps        e_1        e_2        e_3        e_4        e_5        e_6 \n",
      "      4.50       0.25       0.25       0.00       0.00       0.00       0.25 \n",
      "       e_7        e_8        e_9       e_10       e_11     Risk_F \n",
      "      0.00       0.25       0.00       0.25       0.00       1.00 \n",
      "                      new mean [1] \"\\n\"\n",
      "Timestamps        e_1        e_2        e_3        e_4        e_5        e_6 \n",
      "         1          1          1          0          0          0          1 \n",
      "       e_7        e_8        e_9       e_10       e_11     Risk_F \n",
      "         0          1          0          1          0          1 \n",
      "                      mf [1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   1   1   0   0   0   1   0   1   0    1    0      1\n",
      "                      new mf [1] \"\\n\"\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1         10   1   1   0   0   0   1   0   1   0    1    0      0\n",
      "------------------------------------------------2 new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "11         10   1   1   0   0   0   1   0   1   0    1    0 0.0000000\n",
      "------------------------------------------------2 window_df changed to[1] \"\\n\"\n",
      " [1] X1  X2  X3  X4  X5  X6  X7  X8  X9  X10 X11 X12 X13\n",
      "<0 rows> (or 0-length row.names)\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "   Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1           1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2           2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7           6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31          3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41          4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51          5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61          6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71          7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8           8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "11         10   1   1   0   0   0   1   0   1   0    1    0 0.0000000\n",
      "91          9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "------------------------------------------------new_ep[1] \"\\n\"\n",
      "    Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1            1   0   1   0   0   0   1   0   0   0    0    0 0.9975274\n",
      "2            2   1   0   0   0   0   0   0   1   0    0    0 0.9999939\n",
      "3            3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "4            4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "5            5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "6            6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "7            6   1   1   0   0   0   1   0   1   0    1    0 1.0000000\n",
      "31           3   0   0   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "41           4   1   1   0   0   0   0   0   0   0    0    0 1.0000000\n",
      "51           5   0   0   0   0   0   0   0   1   0    1    0 1.0000000\n",
      "61           6   0   0   0   0   0   1   0   0   0    0    0 1.0000000\n",
      "71           7   1   0   0   0   0   0   0   1   0    1    0 0.0000000\n",
      "8            8   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "9            9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "10          10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "11          10   1   1   0   0   0   1   0   1   0    1    0 0.0000000\n",
      "91           9   0   0   0   0   0   1   0   0   0    1    0 0.0000000\n",
      "101         10   0   0   0   0   0   0   0   0   0    0    0 0.0000000\n",
      "                      mean [1] \"\\n\"\n",
      " X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 \n",
      "NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN \n",
      "                      new mean [1] \"\\n\"\n",
      " X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13 \n",
      "NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN \n",
      "                      mf [1] \"\\n\"\n",
      "   X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13\n",
      "1 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n",
      "                      new mf [1] \"\\n\"\n",
      "  X1  X2  X3  X4  X5  X6  X7  X8  X9 X10 X11 X12 X13\n",
      "1 10 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN   0\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in match.names(clabs, names(xi)): names do not match previous names\n",
     "output_type": "error",
     "traceback": [
      "Error in match.names(clabs, names(xi)): names do not match previous names\nTraceback:\n",
      "1. preprocess(training_set, TEST_DATA, REMOVE_RARE_EVENTS, REMOVE_FREQUENT_EVENTS, \n .     KEEP_ONLY_FIRST_OCCURENESS, MULTI_INSTANCE_LEARNING_TEXT, \n .     MULTI_INSTANCE_LEARNING_IMAGE, FEATURE_SELECTION, top_features, \n .     s, midpoint, b_length, target_event, target_event_frequency_proportion_rare, \n .     max_event_frequency_proportion_frequent, milw, F_thres)",
      "2. mil_text(w, F_thres, episodes_list, b_length)   # at line 44 of file <text>",
      "3. rbind(new_ep, mf)   # at line 56 of file <text>",
      "4. rbind(deparse.level, ...)",
      "5. match.names(clabs, names(xi))",
      "6. stop(\"names do not match previous names\")"
     ]
    }
   ],
   "source": [
    "#Preprocessing for training set.\n",
    "\n",
    "TEST_DATA = FALSE\n",
    "REMOVE_RARE_EVENTS = argv$rre\n",
    "REMOVE_FREQUENT_EVENTS = argv$rfe\n",
    "KEEP_ONLY_FIRST_OCCURENESS = argv$kofe\n",
    "MULTI_INSTANCE_LEARNING_TEXT = argv$milt #MIL as explained in the text\n",
    "MULTI_INSTANCE_LEARNING_IMAGE = argv$mili #MIL as presented in the figure\n",
    "FEATURE_SELECTION = argv$fs\n",
    "\n",
    "\n",
    "episodes_list <- preprocess(training_set,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS,KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT,MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION,top_features,s,midpoint,b_length,target_event,target_event_frequency_proportion_rare,max_event_frequency_proportion_frequent,milw,F_thres)\n",
    "if(MULTI_INSTANCE_LEARNING_TEXT==FALSE && MULTI_INSTANCE_LEARNING_IMAGE==FALSE)\n",
    "{\n",
    "    print(\"The returning episodes_list is:\")\n",
    "    print(episodes_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"After changings merged_episodes:\"\n",
      "   e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11    Risk_F\n",
      "1    0   1   0   0   0   1   0   0   0    0    0 0.6681878\n",
      "2    1   0   0   0   0   0   0   1   0    0    0 0.8021839\n",
      "3    0   0   0   0   0   0   0   0   0    0    0 0.8909032\n",
      "4    1   1   0   0   0   0   0   0   0    0    0 0.9426758\n",
      "5    0   0   0   0   0   0   0   1   0    1    0 0.9706878\n",
      "6    0   0   0   0   0   1   0   0   0    0    0 0.9852260\n",
      "7    1   1   0   0   0   1   0   1   0    1    0 0.9852260\n",
      "8    0   0   0   0   0   0   0   0   0    0    0 0.8909032\n",
      "9    1   1   0   0   0   0   0   0   0    0    0 0.9426758\n",
      "10   0   0   0   0   0   0   0   1   0    1    0 0.9706878\n",
      "11   0   0   0   0   0   1   0   0   0    0    0 0.9852260\n",
      "12   1   0   0   0   0   0   0   1   0    1    0 0.9926085\n",
      "13   0   0   0   0   0   0   0   0   0    0    0 0.9963158\n",
      "14   1   1   0   0   0   1   0   1   0    1    0 0.9963158\n",
      "15   0   0   0   0   0   0   0   1   0    1    0 0.9706878\n",
      "16   0   0   0   0   0   1   0   0   0    0    0 0.9852260\n",
      "17   1   0   0   0   0   0   0   1   0    1    0 0.9926085\n",
      "18   0   0   0   0   0   0   0   0   0    0    0 0.9963158\n",
      "19   0   0   0   0   0   1   0   0   0    1    0 0.9981671\n",
      "20   0   0   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "21   1   0   0   0   0   1   0   1   0    1    0 0.9990889\n",
      "22   1   0   0   0   0   0   0   1   0    1    0 0.9926085\n",
      "23   0   0   0   0   0   0   0   0   0    0    0 0.9963158\n",
      "24   0   0   0   0   0   1   0   0   0    1    0 0.9981671\n",
      "25   0   0   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "26   1   0   0   0   0   1   0   1   0    1    0 0.9990889\n",
      "27   0   0   0   0   0   1   0   0   0    1    0 0.9981671\n",
      "28   0   0   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "29   0   0   0   0   0   1   0   0   0    1    0 0.9990889\n",
      "30   0   1   0   0   0   0   0   0   0    0    0 0.9852260\n",
      "31   1   0   0   0   0   1   0   1   0    1    0 0.9926085\n",
      "32   0   1   0   0   0   0   0   0   0    0    0 0.9963158\n",
      "33   0   0   0   0   0   0   0   0   0    0    0 0.9981671\n",
      "34   1   1   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "35   1   1   0   0   0   1   0   1   0    1    0 0.9990889\n",
      "36   0   1   0   0   0   0   0   0   0    0    0 0.9963158\n",
      "37   0   0   0   0   0   0   0   0   0    0    0 0.9981671\n",
      "38   1   1   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "39   1   1   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "40   1   1   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "41   1   1   0   0   0   0   0   0   0    0    0 0.9990889\n",
      "42   0   1   0   0   0   0   0   0   0    0    0 0.9926085\n",
      "43   1   0   0   0   0   1   0   0   0    1    0 0.9963158\n",
      "44   0   1   0   0   0   0   0   1   0    0    0 0.9981671\n",
      "45   0   0   0   0   0   1   0   0   0    0    0 0.9990889\n",
      "46   1   1   0   0   0   1   0   1   0    1    0 0.9990889\n",
      "47   0   1   0   0   0   0   0   1   0    0    0 0.9981671\n",
      "48   0   0   0   0   0   1   0   0   0    0    0 0.9990889\n",
      "49   0   1   0   0   0   1   0   1   0    0    0 0.9990889\n"
     ]
    }
   ],
   "source": [
    "merged_episodes = ldply(episodes_list, data.frame) #merging all lists of episodes list to one data frame\n",
    "merged_episodes = merged_episodes[ , !(names(merged_episodes) %in% c(\"Timestamps\"))] #delete column Timestamps\n",
    "\n",
    "print(\"After changings merged_episodes (the data that will fit the model):\")\n",
    "print(merged_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection function\n",
    "\n",
    "**function: feature_selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for selectiong the top features.\n",
    "\n",
    "feature_selection <- function(merged_episodes,top_features){\n",
    "  #Feature selection using reliefF\n",
    "    \n",
    "  #attrEval function -> https://www.rdocumentation.org/packages/CORElearn/versions/1.53.1/topics/attrEval  \n",
    "  #ReliefF -> https://medium.com/@yashdagli98/feature-selection-using-relief-algorithms-with-python-example-3c2006e18f83      \n",
    "  estReliefF <- attrEval(Risk_F ~ ., merged_episodes, estimator=\"RReliefFexpRank\", ReliefIterations=50)\n",
    "   \n",
    "  #sort indeces of  estReliefF   \n",
    "  sorted_indeces = order(estReliefF, decreasing = TRUE)\n",
    "    \n",
    "  #keep the the top (top_features) \"useful\" columns of instances data frame  \n",
    "  merged_episodes = merged_episodes %>% select(sorted_indeces[1:top_features],ncol(merged_episodes))\n",
    "    \n",
    "  return(merged_episodes)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(FEATURE_SELECTION){\n",
    "  #remove columns with all values equal to zero\n",
    "  merged_episodes = merged_episodes[, colSums(merged_episodes != 0) > 0]\n",
    "  \n",
    "  #keep the top features\n",
    "  merged_episodes = feature_selection(merged_episodes,top_features)\n",
    "    \n",
    "  print(\"Merged_episodes after feature selection:\")\n",
    "  print(merged_episodes)  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The starting episodes_list is:\"\n",
      "[[1]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   1   0   1   2   1   1   2   0   0    0    0      1\n",
      "2          2   1   1   1   1   2   0   1   0   0    0    0      1\n",
      "3          3   1   1   1   1   1   2   0   1   0    0    0      1\n",
      "4          4   0   0   1   1   2   0   1   0   0    1    0      1\n",
      "5          5   1   1   2   1   1   2   1   0   1    0    0      1\n",
      "6          6   1   1   0   1   1   1   2   0   0    0    0      1\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   2   1   1   1   1   0   1    0    0      1\n",
      "2          2   1   1   1   1   1   0   1   0   0    1    0      1\n",
      "3          3   0   0   1   2   0   0   2   0   0    0    0      1\n",
      "\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After binirizing:\"\n",
      "[[1]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   1   0   1   1   1   1   1   0   0    0    0      1\n",
      "2          2   1   1   1   1   1   0   1   0   0    0    0      1\n",
      "3          3   1   1   1   1   1   1   0   1   0    0    0      1\n",
      "4          4   0   0   1   1   1   0   1   0   0    1    0      1\n",
      "5          5   1   1   1   1   1   1   1   0   1    0    0      1\n",
      "6          6   1   1   0   1   1   1   1   0   0    0    0      1\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   1   1   1   1   1   0   1    0    0      1\n",
      "2          2   1   1   1   1   1   0   1   0   0    1    0      1\n",
      "3          3   0   0   1   1   0   0   1   0   0    0    0      1\n",
      "\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The returning test_episodes_list (data for prediction) is:\"\n",
      "[[1]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   1   0   1   1   1   1   1   0   0    0    0      1\n",
      "2          2   1   1   1   1   1   0   1   0   0    0    0      1\n",
      "3          3   1   1   1   1   1   1   0   1   0    0    0      1\n",
      "4          4   0   0   1   1   1   0   1   0   0    1    0      1\n",
      "5          5   1   1   1   1   1   1   1   0   1    0    0      1\n",
      "6          6   1   1   0   1   1   1   1   0   0    0    0      1\n",
      "\n",
      "[[2]]\n",
      "  Timestamps e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1          1   0   1   1   1   1   1   1   0   1    0    0      1\n",
      "2          2   1   1   1   1   1   0   1   0   0    1    0      1\n",
      "3          3   0   0   1   1   0   0   1   0   0    0    0      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for testing set.\n",
    "\n",
    "TEST_DATA = TRUE\n",
    "REMOVE_RARE_EVENTS = FALSE\n",
    "REMOVE_FREQUENT_EVENTS = FALSE\n",
    "KEEP_ONLY_FIRST_OCCURENESS = FALSE\n",
    "MULTI_INSTANCE_LEARNING_TEXT = FALSE #MIL as explained in the text\n",
    "MULTI_INSTANCE_LEARNING_IMAGE = FALSE #MIL as presented in the figure\n",
    "FEATURE_SELECTION = FALSE\n",
    "\n",
    "test_episodes_list <- preprocess(test_set,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS,KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT,MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION,top_features,s,midpoint,b_length,target_event,target_event_frequency_proportion_rare,max_event_frequency_proportion_frequent,milw,F_thres)\n",
    "\n",
    "print(\"The returning test_episodes_list (data for prediction) is:\")\n",
    "print(test_episodes_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Random Forest \n",
    "\n",
    "The segmented data in combination\n",
    "with the risk quantification values are fed into a **Random Forests** algorithm as\n",
    "a training set to form a regression problem, which is based on the minimization\n",
    "of the mean squared error.\n",
    "\n",
    "**1) function: eval**\n",
    "\n",
    "**2) function: plot (the 2nd of XGBoost's functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that uses Random Forest for predictive the target event(i.e. fault) and evaluating the results.\n",
    "\n",
    "eval <- function(train_episodes,test_episodes_list,seed,plotbool){\n",
    "  set.seed(seed) #for remaining the random output the same\n",
    "    \n",
    "  #Training with randomForest model using instances_df(made from training set)\n",
    "  #Random Forest R documentation and parameters explanation -> https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest  \n",
    "  #Random Forest idea -> https://www.youtube.com/watch?v=loNcrMjYh64  \n",
    "  my.rf <- randomForest(Risk_F ~ .,data=train_episodes,importance=TRUE, ntree=2) #(default ntree=500) \n",
    "    \n",
    "  #if ploot then print the tree informations    \n",
    "  if(plotbool){  \n",
    "    varImpPlot(my.rf)  \n",
    "  }\n",
    "    \n",
    "\n",
    "  \n",
    "  #if plootbool then print the tree informations  \n",
    "  if(plotbool==TRUE)\n",
    "  {\n",
    "   print(\"------------------------------------------------------------------------------------------------------\")   \n",
    "   for(i in 1:2){   \n",
    "        tryCatch(\n",
    "          expr = {\n",
    "            print(getTree(my.rf, i,labelVar=TRUE))\n",
    "          },\n",
    "          error = function(e){ \n",
    "            \n",
    "          }\n",
    "        )  \n",
    "   } \n",
    "   print(\"------------------------------------------------------------------------------------------------------\")   \n",
    "  }   \n",
    "    \n",
    "    \n",
    "    \n",
    "  #library(tree)\n",
    "  #tr <- tree(my.rf)\n",
    "  #print(tr$frame)\n",
    "\n",
    "  false_positives = 0\n",
    "  true_positives = 0\n",
    "  false_negatives = 0\n",
    "  i=0\n",
    "  \n",
    "  #for every episode in the episodes_list  \n",
    "  for(ep in test_episodes_list){\n",
    "\n",
    "    ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "    \n",
    "    print(\"For episode:\")\n",
    "    print(ep)\n",
    "    \n",
    "    Prediction <- predict(my.rf, ep) #make the prediction of the episode\n",
    "\n",
    "    print(\"Prediction is:\")\n",
    "    print(Prediction)\n",
    "    \n",
    "\n",
    "    ep_legth = length(Prediction)\n",
    "    \n",
    "    print(\"Prediction ep_length is:\")\n",
    "    print(ep_legth)\n",
    "    \n",
    "    #keep the positions of the TRUEs  \n",
    "    #TRUE if prediction is bigger than the acceptance threshold,else FALSE\n",
    "    pred_indeces = as.numeric(names(Prediction[Prediction >= acceptance_threshold])) #acceptance_threshold=0.5\n",
    "    \n",
    "    print(\"The pred_indeces:\")\n",
    "    print(pred_indeces)\n",
    "    \n",
    "    #if there is warnings before the max interval from the failure(target event)  \n",
    "    if(length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))]) > 0){\n",
    "      #increase false positives by the number of these warnings    \n",
    "      false_positives = false_positives + length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))])\n",
    "    } \n",
    "      \n",
    "    #if there is warnings after the max and before the min interval from the failure(target event)     \n",
    "    if(length(pred_indeces[pred_indeces >= (ep_legth-(max_warning_interval)) & pred_indeces <= (ep_legth-min_warning_interval)]) > 0){\n",
    "      true_positives = true_positives + 1 #increase true positives by 1\n",
    "    #if there is no correct warning    \n",
    "    } else {\n",
    "      false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "    }\n",
    "      \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")    \n",
    "  }\n",
    "\n",
    "  \n",
    "  precision = true_positives/(true_positives+false_positives) #calculate the precision of the model\n",
    "  if((true_positives+false_positives) == 0){\n",
    "    precision = 0\n",
    "  }\n",
    "  recall = true_positives/length(test_episodes_list) #calculate recall of the model\n",
    "  \n",
    "  F1 = 2*((precision*recall)/(precision+recall)) #calculate F1 score of the model\n",
    "  if((precision+recall) == 0){\n",
    "    F1 = 0\n",
    "  }\n",
    "\n",
    "  #prints \n",
    "  if(!csv){\n",
    "    cat(paste(\"dataset:\",argv$test,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\", false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\", precision,\"\\nrecall:\", recall,\"\\nF1:\", F1, \"\\n\"))\n",
    "  } else {\n",
    "    cat(paste(argv$test,\",\",true_positives,\",\",false_positives,\",\",false_negatives,\",\",precision,\",\",recall,\",\",F1,\",\",argv$fet,\",\",argv$tet,\",\",argv$rre,\",\",argv$rfe,\",\",argv$kofe,\",\",argv$mili,\",\",argv$milt,\",\",argv$fs,\",\",argv$top,\",\",argv$rer,\",\",argv$fer,\",\",argv$seed,\",\",argv$steepness,\",\",argv$pthres,\",\",argv$milw,\",\",argv$milthres,\",\",argv$midpoint,\",\",argv$minwint,\",\",argv$maxwint,\"\\n\",sep=\"\"))\n",
    "  }\n",
    "    \n",
    "  print(\"F1 score is:\")\n",
    "  print(F1)\n",
    "  return(my.rf)\n",
    "}\n",
    "\n",
    "\n",
    "#plot function\n",
    "plot <- function(test_episodes_list, episode_index, my.rf){\n",
    "  test_episodes = test_episodes_list[[episode_index]][ , !(names(test_episodes_list[[episode_index]]) %in% c(\"Timestamps\"))]\n",
    "  Prediction <- predict(my.rf, test_episodes)\n",
    "  results = data.frame(Risk_F=test_episodes$Risk_F,num_Prediction=as.numeric(Prediction))\n",
    "  mse = mean((Prediction-test_episodes$Risk_F)^2)\n",
    "  \n",
    "  chart =ggplot(results,aes((1:nrow(results)))) +\n",
    "    # geom_rect(aes(xmin = ceiling(nrow(df_test)/2), xmax = nrow(df_test), ymin = -Inf, ymax = Inf),\n",
    "    #           fill = \"yellow\", alpha = 0.003) +\n",
    "    geom_line(aes(y = Risk_F, colour = \"Actual\")) +\n",
    "    geom_line(aes(y = num_Prediction, colour=\"Predicted\")) +\n",
    "    labs(colour=\"Lines\") +\n",
    "    xlab(\"Segments\") +\n",
    "    ylab('Risk (F)') +\n",
    "    ggtitle(\"Risk Prediction\") + # (RR_KF_2YEARS_PAT08)\n",
    "    theme(plot.title = element_text(hjust = 0.5)) +\n",
    "    geom_text(aes(label = paste(\"MSE=\",round(mse,3)), x = 20, y = 1), hjust = -2, vjust = 6, color=\"black\", size=4) #add MSE label\n",
    "  \n",
    "  # Disable clip-area so that the MSE is shown in the plot\n",
    "  gt <- ggplot_gtable(ggplot_build(chart))\n",
    "  gt$layout$clip[gt$layout$name == \"panel\"] <- \"off\"\n",
    "  grid.draw(gt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "  left daughter right daughter split var split point status prediction\n",
      "1             2              3       e_1         0.5     -3  0.9739620\n",
      "2             0              0      <NA>         0.0     -1  0.9673472\n",
      "3             4              5      e_10         0.5     -3  0.9798135\n",
      "4             0              0      <NA>         0.0     -1  0.9606746\n",
      "5             0              0      <NA>         0.0     -1  0.9938487\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"For episode:\"\n",
      "  e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1   1   0   1   1   1   1   1   0   0    0    0      1\n",
      "2   1   1   1   1   1   0   1   0   0    0    0      1\n",
      "3   1   1   1   1   1   1   0   1   0    0    0      1\n",
      "4   0   0   1   1   1   0   1   0   0    1    0      1\n",
      "5   1   1   1   1   1   1   1   0   1    0    0      1\n",
      "6   1   1   0   1   1   1   1   0   0    0    0      1\n",
      "[1] \"Prediction is:\"\n",
      "        1         2         3         4         5         6 \n",
      "0.9618716 0.9618716 0.9618716 0.9652078 0.9618716 0.9618716 \n",
      "[1] \"Prediction ep_length is:\"\n",
      "[1] 6\n",
      "[1] \"The pred_indeces:\"\n",
      "[1] 1 2 3 4 5 6\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"For episode:\"\n",
      "  e_1 e_2 e_3 e_4 e_5 e_6 e_7 e_8 e_9 e_10 e_11 Risk_F\n",
      "1   0   1   1   1   1   1   1   0   1    0    0      1\n",
      "2   1   1   1   1   1   0   1   0   0    1    0      1\n",
      "3   0   0   1   1   0   0   1   0   0    0    0      1\n",
      "[1] \"Prediction is:\"\n",
      "        1         2         3 \n",
      "0.9652078 0.9784586 0.9652078 \n",
      "[1] \"Prediction ep_length is:\"\n",
      "[1] 3\n",
      "[1] \"The pred_indeces:\"\n",
      "[1] 1 2 3\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "C:/Users/Public/ptyxiakh/testing_my_dataset.csv,2,0,0,1,1,1,11,11,TRUE,TRUE,TRUE,FALSE,TRUE,FALSE,3,0.5,0.8,400,0.7,0.5,6,0.4,11,2,5\n",
      "[1] \"F1 score is:\"\n",
      "[1] 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAABNTU1oaGh8fHyM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\nBCggJEABIQEKCAlQQEiAAkICFPwHk8tcTr4jFQQAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Run Random Forest algorithm and print the results.\n",
    "\n",
    "my.rf = eval(merged_episodes,test_episodes_list,seed,TRUE)\n",
    "\n",
    "#plot saving as image\n",
    "# for(s in (0:6)){\n",
    "#   my.rf = eval(merged_episodes,test_episodes_list,seed,FALSE)\n",
    "#   seed = seed + 1\n",
    "# }\n",
    "\n",
    "# for(ep in 1:length(test_episodes_list)){\n",
    "#   jpeg(paste(ep,'_rplot.jpg'))\n",
    "#   plot(test_episodes_list,ep,my.rf)\n",
    "#   dev.off()\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Sequential pattern mining(SPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for creating output for Hirate Yamana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_ds_for_spm <- function(target_event,episodes_list,output){\n",
    "  if (file.exists(output)) {\n",
    "    file.remove(output)\n",
    "  }\n",
    "  #output for HirateYamana\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    ep = episodes_list[[ep_index]][ , !(names(episodes_list[[ep_index]]) %in% c(\"Timestamps\"))]\n",
    "    ep_list = list()\n",
    "    for(i in (1:nrow(ep))){\n",
    "      matches = which(ep[i,] %in% c(1))\n",
    "      if(length(matches) == 0){\n",
    "        next\n",
    "      }\n",
    "      line=paste(matches,collapse=\" \")\n",
    "      ep_list[i] = line\n",
    "    }\n",
    "    if(length(ep_list) == 0){\n",
    "      next\n",
    "    }\n",
    "    ep_list[length(ep_list)+1] = target_event\n",
    "    episode = \"\"\n",
    "    for(ep_lli in (1:length(ep_list))){\n",
    "      if(length(ep_list[[ep_lli]]) > 0){\n",
    "        index = paste(paste(\"<\",ep_lli-1,sep=\"\"),\">\",sep=\"\") #I CHANGED IT FROM ep_lli to ep_lli-1\n",
    "        if(episode == \"\"){\n",
    "          episode = paste(index,ep_list[[ep_lli]],sep=\" \")\n",
    "        } else {\n",
    "          episode = paste(episode,paste(index,ep_list[[ep_lli]],sep=\" \"),sep=\" -1 \")\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    write(paste(episode,\"-1 -2\"),file=output,append=TRUE)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the necessary variables for SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(export_spm){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~SEQUENTIAL PATTERN MINING~~~~~~~\")\n",
    "  }\n",
    "    spm_train_path = gsub(\".csv\",paste(\"_spm_\",id,\".csv\",sep=\"\"),argv$train)\n",
    "    spm_test_path = gsub(\".csv\",paste(\"_spm_\",id,\".csv\",sep=\"\"),argv$test)\n",
    "    spm_results_path = gsub(\".csv\",paste(\"_results_\",id,\".csv\",sep=\"\"),argv$train)\n",
    "    \n",
    "    \n",
    "    target_event = argv$tet\n",
    "\n",
    "    confidence = argv$conf\n",
    "\n",
    "    min_dist_seq = argv$minti\n",
    "    max_dist_seq = argv$maxti\n",
    "\n",
    "    min_dist_first_last = argv$minwi\n",
    "    max_dist_first_last = argv$maxwi\n",
    "\n",
    "    max_warning_interval = argv$maxwint\n",
    "    min_warning_interval = argv$minwint\n",
    "\n",
    "    java_path = argv$java\n",
    "    jspmf_path = argv$spmf\n",
    "\n",
    "    python_path = argv$python\n",
    "    cep_path = argv$cep\n",
    "}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting output(as .csv file) for Hirate Yamana "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(export_spm){   \n",
    "    export_ds_for_spm(target_event,episodes_list,spm_train_path)\n",
    "    export_ds_for_spm(target_event,test_episodes_list,spm_test_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run HirateYamana algorithm \n",
    "\n",
    "Using a jar file named **spmf.jar**(jspmf_path) the **HirateYamana** algorithm is running for the **spm_train_path**(.csv file of the training set). \n",
    "\n",
    "Hirate Yamana: https://www.philippe-fournier-viger.com/spmf/hirateyamana.pdf\n",
    "\n",
    "Hirate Yamana example: https://www.philippe-fournier-viger.com/spmf/HirateYamana.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(export_spm){    \n",
    "    if (file.exists(spm_results_path)) {\n",
    "      invisible(file.remove(spm_results_path))\n",
    "    }\n",
    "\n",
    "    javaOutput <- system(paste(java_path,\"-jar\",jspmf_path,\"run HirateYamana\",spm_train_path,spm_results_path,confidence,min_dist_seq,max_dist_seq,min_dist_first_last,max_dist_first_last), intern = TRUE)\n",
    "\n",
    "    print(\"The .csv file of the HirateYamanas' results looks like:\")\n",
    "    hiryamres <- read.csv(file = spm_results_path,header=FALSE)\n",
    "    head(hiryamres)\n",
    " \n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Rules and make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(export_spm){    \n",
    "    pythonOutput <- system(paste(python_path,cep_path,spm_results_path,spm_test_path,target_event), intern = TRUE)\n",
    "\n",
    "    print(\"The python output is:\")\n",
    "    print(pythonOutput)\n",
    "\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate spm results\n",
    "\n",
    "Calculating recall, precion and F1 score for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(export_spm){    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    total_failures = 0\n",
    "\n",
    "    day = 0\n",
    "\n",
    "    warnings = list()\n",
    "\n",
    "    ep_count=1\n",
    "\n",
    "    #for every line of pythonOutput\n",
    "    for(w in pythonOutput){ \n",
    "      #if string \"Warning\" appears in the line  \n",
    "      if(grepl(\"Warning \",w,fixed=TRUE)){\n",
    "        day = as.integer(str_extract(w, \"\\\\-*\\\\d+\\\\.*\\\\d*\")) #day's serial number  \n",
    "        warnings = c(warnings,day) #\n",
    "        #print(\"Waring list is:\")\n",
    "        #print(warnings)  \n",
    "      #if string \"Failure\" appears in the line     \n",
    "      } else if(grepl(\"Failure\",w,fixed=TRUE)){  \n",
    "\n",
    "        print(\"-------------------\")  \n",
    "        print(\"For episode:\")\n",
    "        print(ep_count) \n",
    "        #print(\"-------------------\")\n",
    "        ep_count=ep_count+1  \n",
    "\n",
    "        day = as.integer(str_extract(w, \"\\\\-*\\\\d+\\\\.*\\\\d*\")) #day's serial number \n",
    "\n",
    "        total_failures = total_failures + 1 #increase total failures by 1\n",
    "\n",
    "        day=day-1\n",
    "\n",
    "        print(\"The serial number of the day, when the failure(target event) happens is:\")\n",
    "        print(day)  #day or day-1?\n",
    "        #print(\"-------------------\")\n",
    "\n",
    "\n",
    "        #if there is no warning  \n",
    "        if(length(warnings) == 0){\n",
    "          false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "        #if there is warning(s)    \n",
    "        } else {\n",
    "          if(length(warnings[warnings < day-max_warning_interval]) > 0){\n",
    "            #increase false positives by the number of these warnings\n",
    "            false_positives = false_positives + length(warnings[warnings < day-max_warning_interval]) \n",
    "          }\n",
    "\n",
    "          #if there is warnings after the max and before the min interval from the failure(target event)   \n",
    "          if(length(warnings[warnings >= (day-max_warning_interval)]) > 0 & length(warnings[warnings <= (day-min_warning_interval)]) > 0){\n",
    "            true_positives = true_positives + 1 #increase true positives by 1\n",
    "          #if there is no correct warning    \n",
    "          } else {\n",
    "            false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "          }\n",
    "        }\n",
    "        warnings = list() #empty the list\n",
    "      }\n",
    "    }\n",
    "\n",
    "    precision = true_positives/(true_positives+false_positives) #calculate the precision of the model\n",
    "    if((true_positives+false_positives) == 0){\n",
    "      precision = 0\n",
    "    }\n",
    "\n",
    "    recall = true_positives/total_failures #calculate recall of the model\n",
    "\n",
    "    F1 = 2*((precision*recall)/(precision+recall)) #calculate F1 score of the model\n",
    "    if(is.na((precision+recall)<=0.00)){\n",
    "      F1 = 0\n",
    "    }\n",
    "\n",
    "    #prints\n",
    "    #if(!csv){\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    if(TRUE){    \n",
    "      cat(paste(\"dataset:\",argv$test,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\", false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\", precision,\"\\nrecall:\", recall,\"\\nF1:\", F1, \"\\n\"))\n",
    "    } else {\n",
    "      cat(paste(argv$test,\",\", true_positives,\",\", false_positives,\",\", false_negatives,\",\", precision,\",\", recall,\",\", F1,\",\",argv$conf,\",\",argv$minti,\",\",argv$maxti,\",\",argv$minwi,\",\",argv$maxwi,\",\",argv$minwint,\",\",argv$maxwint, \"\\n\",sep=\"\"))\n",
    "    }\n",
    "    \n",
    "}    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
